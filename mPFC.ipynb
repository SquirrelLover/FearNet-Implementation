{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIHnhKcUo/s1CicUBIPllX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SquirrelLover/FearNet-Implementation/blob/main/mPFC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50"
      ],
      "metadata": {
        "id": "NAa1Nl-k21SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSrfs1vh2X_X",
        "outputId": "87dc41c0-eecd-4f9a-b6f8-2ed77587579e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Mini_batch = 450\n",
        "Dropout = 0.25\n",
        "Learning_Rate = 5e-4"
      ],
      "metadata": {
        "id": "0yTRBfMI1VgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset (CIFAR-100)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR100(root='./data', train=True, transform = transform, download = True)\n",
        "test_data = torchvision.datasets.CIFAR100(root='./data', train=False, transform = transform, download = True)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = Mini_batch, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size = Mini_batch, shuffle=False)\n",
        "\n",
        "# view a couple of sample images to make sure they are loaded\n",
        "\n",
        "print(\"Train Loader Images:\")\n",
        "images, labels = next(iter(train_loader))\n",
        "images = images.detach().numpy()\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  plt.subplot(1, 4, i+1)\n",
        "  plt.imshow(np.transpose(images[i], (1, 2, 0)))\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"Image of class {labels[i].item()}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "gjJ6jJsagCvO",
        "outputId": "ca73c23b-44d8-4317-eae4-93dee7ec6a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loader Images:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet processing\n",
        "\n",
        "resnet = resnet50(pretrained=True)\n",
        "resnet = resnet.to(device)\n",
        "resnet.eval()\n",
        "\n",
        "resnet_feature_extractor = nn.Sequential(*list(resnet.children())[:-1]).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvJX8qO26DEb",
        "outputId": "6e1f17bd-2bb8-4c64-945c-16fced578315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass data through Resnet processing\n",
        "class CIFAR100FeatureDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        return image, label\n",
        "\n",
        "# training data\n",
        "num_images = len(train_data)\n",
        "feature_vectors = torch.zeros((num_images, 2048), device=device)\n",
        "labels = torch.zeros(num_images, device=device)\n",
        "\n",
        "feature_dataset = CIFAR100FeatureDataset(train_data)\n",
        "feature_loader = DataLoader(feature_dataset, batch_size=Mini_batch, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels_batch) in enumerate(feature_loader):\n",
        "        images = images.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "        features_batch = resnet_feature_extractor(images).squeeze()\n",
        "        start_index = i * Mini_batch\n",
        "        end_index = start_index + features_batch.shape[0]\n",
        "        feature_vectors[start_index:end_index] = features_batch\n",
        "        labels[start_index:end_index] = labels_batch\n",
        "\n",
        "feature_vectors = torch.tensor(feature_vectors, dtype=torch.float32).to(device)\n",
        "labels = torch.tensor(labels).long().to(device)\n",
        "\n",
        "print(\"Training features shape:\", feature_vectors.shape)\n",
        "print(\"Training labels shape:\", labels.shape)\n",
        "\n",
        "# testing data\n",
        "num_images = len(test_data)\n",
        "test_feature_vectors = torch.zeros((num_images, 2048), device=device)\n",
        "test_labels = torch.zeros(num_images, device=device)\n",
        "\n",
        "test_feature_dataset = CIFAR100FeatureDataset(test_data)\n",
        "test_feature_loader = DataLoader(test_feature_dataset, batch_size=Mini_batch, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels_batch) in enumerate(test_feature_loader):\n",
        "        images = images.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "        features_batch = resnet_feature_extractor(images).squeeze()\n",
        "        start_index = i * Mini_batch\n",
        "        end_index = start_index + features_batch.shape[0]\n",
        "        test_feature_vectors[start_index:end_index] = features_batch\n",
        "        test_labels[start_index:end_index] = labels_batch\n",
        "\n",
        "test_feature_vectors = torch.tensor(test_feature_vectors, dtype=torch.float32).to(device)\n",
        "test_labels = torch.tensor(test_labels).long().to(device)\n",
        "\n",
        "print(\"Test features shape:\", test_feature_vectors.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)"
      ],
      "metadata": {
        "id": "KJZAPVHnKgwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUxN0nB320I6"
      },
      "outputs": [],
      "source": [
        "# define mPFC (long term)\n",
        "\n",
        "class mPFC(nn.Module):\n",
        "  def __init__(self, input_dim=2048, autoencoder_hidden_dims = np.array([1024, 512]), classifier_dims = np.array([1024, 512]), lambda_values=[1e4, 1.0, 0.1], learning_rate=5e-4):\n",
        "    super(mPFC, self).__init__()\n",
        "    # encoder\n",
        "    encoder_layers = []\n",
        "    current_dim = input_dim\n",
        "    encoder_layers.append(nn.ELU())\n",
        "    for hidden_dim in autoencoder_hidden_dims:\n",
        "      encoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "      encoder_layers.append(nn.ELU())\n",
        "      current_dim = hidden_dim\n",
        "    self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "    # decoder\n",
        "    decoder_layers = []\n",
        "    hidden_dims_reversed = list(autoencoder_hidden_dims[::-1])\n",
        "    for hidden_dim in hidden_dims_reversed:\n",
        "      decoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "      decoder_layers.append(nn.ELU())\n",
        "      current_dim = hidden_dim\n",
        "    decoder_layers.append(nn.Linear(current_dim, input_dim))\n",
        "    decoder_layers.append(nn.ELU())\n",
        "    self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    #classifier\n",
        "    current_dim = autoencoder_hidden_dims[-1]\n",
        "    classifier_layers= []\n",
        "\n",
        "    if len(classifier_dims) != 1:\n",
        "      classifier_layers.append(nn.Linear(current_dim, classifier_dims[0]))\n",
        "      classifier_layers.append(nn.ELU())\n",
        "      current_dim = classifier_dims[0]\n",
        "\n",
        "      for i in range(1, len(classifier_dims)-1):\n",
        "        hidden_dim = classifier_dims[i]\n",
        "        classifier_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "        classifier_layers.append(nn.ELU())\n",
        "        classifier_layers.append(nn.Dropout(p=Dropout))\n",
        "        current_dim = hidden_dim\n",
        "\n",
        "    classifier_layers.append(nn.Linear(current_dim, classifier_dims[len(classifier_dims)-1]))\n",
        "\n",
        "    self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    # lambda\n",
        "    self.lambda_values = torch.tensor(lambda_values if lambda_values else [1.0] * len(autoencoder_hidden_dims), dtype=torch.float32)\n",
        "\n",
        "    # optimizer\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "  def encoder_forward(self, x):\n",
        "    encoder_intermediates = [x]\n",
        "    for layer in self.encoder:\n",
        "      x = layer(x)\n",
        "      encoder_intermediates.append(x)\n",
        "    encoded = encoder_intermediates[-1]\n",
        "    return encoded, encoder_intermediates\n",
        "\n",
        "  def decoder_forward(self, x):\n",
        "    decoder_intermediates = [x]\n",
        "    for layer in self.decoder:\n",
        "      x = layer(x)\n",
        "      decoder_intermediates.append(x)\n",
        "    pseudo_img = decoder_intermediates[-1]\n",
        "    return pseudo_img, list(decoder_intermediates[::-1])\n",
        "\n",
        "  def classify(self, x):\n",
        "    class_logits = self.classifier(x)\n",
        "    return class_logits\n",
        "\n",
        "  def compute_loss(self, class_logits, targets, encoder_intermediates, decoder_intermediates):\n",
        "    # classification loss\n",
        "    classification_loss = nn.CrossEntropyLoss()(class_logits, targets)\n",
        "\n",
        "    # reconstruction loss with lambda weighting\n",
        "    reconstruction_loss = 0\n",
        "    for i in range(len(self.lambda_values)):\n",
        "      encoder_hidden = encoder_intermediates[i]\n",
        "      decoder_hidden = decoder_intermediates[i]\n",
        "      diff = encoder_hidden - decoder_hidden\n",
        "      squared_diff = diff.pow(2)\n",
        "      layer_loss = squared_diff.sum()\n",
        "      reconstruction_loss += self.lambda_values[i] * layer_loss\n",
        "\n",
        "    # total loss\n",
        "    total_loss = classification_loss + reconstruction_loss\n",
        "\n",
        "    return classification_loss, reconstruction_loss, total_loss\n",
        "\n",
        "  def training_step(self, data_loader, device):\n",
        "    self.train()\n",
        "\n",
        "    total_classification_loss = 0\n",
        "    total_reconstruction_loss = 0\n",
        "    total_total_loss = 0\n",
        "\n",
        "    for x, targets in data_loader:\n",
        "      x, targets = x.to(device), targets.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      encoded, encoder_intermediates = self.encoder_forward(x)\n",
        "      pseudo_img, decoder_intermediates = self.decoder_forward(encoded)\n",
        "      class_logits = self.classify(encoded)\n",
        "      # compute losses\n",
        "      classification_loss, reconstruction_loss, total_loss = self.compute_loss(\n",
        "        class_logits, targets, encoder_intermediates, decoder_intermediates\n",
        "      )\n",
        "\n",
        "      # zero gradients\n",
        "      self.optimizer.zero_grad()\n",
        "      for param in self.decoder.parameters():\n",
        "        param.grad = None\n",
        "      for param in self.encoder.parameters():\n",
        "        param.grad = None\n",
        "\n",
        "      # backward pass\n",
        "      classification_loss.backward(retain_graph=True)\n",
        "      classifier_grads = {name: param.grad.clone() for name, param in self.classifier.named_parameters()}\n",
        "      reconstruction_loss.backward(retain_graph=True)\n",
        "      decoder_grads = {name: param.grad.clone() for name, param in self.decoder.named_parameters()}\n",
        "      total_loss.backward()\n",
        "      for name, param in self.encoder.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = param.grad.data\n",
        "\n",
        "      # optimizer\n",
        "      encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      encoder_optimizer.step()\n",
        "      classifier_optimizer = optim.Adam(self.classifier.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      classifier_optimizer.step()\n",
        "      decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      decoder_optimizer.step()\n",
        "\n",
        "      # update parameters\n",
        "      for name, param in self.classifier.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = classifier_grads[name].data\n",
        "      for name, param in self.decoder.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = decoder_grads[name].data\n",
        "\n",
        "      # add losses to total\n",
        "      total_classification_loss += classification_loss.item()\n",
        "      total_reconstruction_loss += reconstruction_loss.item()\n",
        "      total_total_loss += total_loss.item()\n",
        "\n",
        "    # average loss\n",
        "    average_classification_loss = total_classification_loss / len(data_loader)\n",
        "    average_reconstruction_loss = total_reconstruction_loss / len(data_loader)\n",
        "    average_total_loss = total_total_loss / len(data_loader)\n",
        "\n",
        "    return average_classification_loss, average_reconstruction_loss, average_total_loss\n",
        "\n",
        "  def generate_statistics(self, list_feature_vectors, device):\n",
        "    self.eval()\n",
        "\n",
        "    latent_rep = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, targets in list_feature_vectors:\n",
        "        x, targets = x.to(device), targets.to(device)\n",
        "\n",
        "        # pass through encoder\n",
        "        mPFC_out, _ = self.encoder_forward(x)\n",
        "        latent_rep.append(mPFC_out.cpu())\n",
        "\n",
        "    # stack\n",
        "    latent_rep = torch.cat(latent_rep, dim=0)\n",
        "\n",
        "    # ux\n",
        "    ux_mPFC = latent_rep.mean(dim=0)\n",
        "\n",
        "    # covariance matrix\n",
        "    centered_mPFC_features = latent_rep - ux_mPFC\n",
        "    covariance_matrix_mPFC = torch.matmul(centered_mPFC_features.t(), centered_mPFC_features) / (latent_rep.size(0) - 1)\n",
        "\n",
        "    return ux_mPFC, covariance_matrix_mPFC\n",
        "\n",
        "  def pseudoimg_from_statistics(self, u, covar, count):\n",
        "    self.eval()\n",
        "\n",
        "    # sampling from a multivariate normal distribution\n",
        "    distribution = torch.distributions.MultivariateNormal(u, covar)\n",
        "    sampled_vectors = distribution.sample((count,)).to(device)\n",
        "\n",
        "    # pass through decoder\n",
        "    pseudo_images = []\n",
        "    with torch.no_grad():\n",
        "      for latent_vector in sampled_vectors:\n",
        "        pseudo_img, _ = self.decoder_forward(latent_vector)\n",
        "        pseudo_images.append(pseudo_img.cpu())\n",
        "\n",
        "    # stack\n",
        "    pseudo_images = torch.stack(pseudo_images, dim=0)\n",
        "\n",
        "    return pseudo_images\n",
        "\n",
        "  def calculate_accuracy(self, data_loader, device):\n",
        "    self.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, targets in data_loader:\n",
        "        x, targets = x.to(device), targets.to(device)\n",
        "        encoded, _ = self.encoder_forward(x)\n",
        "        class_logits = self.classify(encoded)\n",
        "        _, predicted = torch.max(class_logits.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = mPFC()\n",
        "model = model.to(device)\n",
        "\n",
        "#print(summary(model.encoder, input_size=(3072,)))\n",
        "#print(summary(model.decoder, input_size=(256,)))\n",
        "\n",
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels.long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "        return feature, label\n",
        "\n",
        "dataset = FeatureDataset(feature_vectors, labels)\n",
        "\n",
        "for epoch in range(30):\n",
        "    data_loader = DataLoader(dataset, batch_size=Mini_batch, shuffle=True)\n",
        "\n",
        "    avg_classification_loss, avg_reconstruction_loss, avg_total_loss = model.training_step(data_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Total Loss: {avg_total_loss}, Classification Loss: {avg_classification_loss}, Reconstruction Loss: {avg_reconstruction_loss}\")\n",
        "    #for param_group in model.optimizer.param_groups:\n",
        "    #    print(\"Learning rate:\", param_group['lr'])\n",
        "    #for name, param in model.named_parameters():\n",
        "    #    if param.grad is not None:\n",
        "    #        print(f\"{name} gradient norm: {param.grad.norm().item()}\")\n",
        "\n",
        "    accuracy = model.calculate_accuracy(data_loader, device)\n",
        "    print(f\"Epoch {epoch+1}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "BzxGXR78gedJ",
        "outputId": "6eda5ea3-b8e6-4c34-edfd-1897d1b95847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-aae312849412>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mavg_classification_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reconstruction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_total_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}, Total Loss: {avg_total_loss}, Classification Loss: {avg_classification_loss}, Reconstruction Loss: {avg_reconstruction_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-7e7cb1faa508>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, data_loader, device)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0mdecoder_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m       \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test accuracy\n",
        "\n",
        "test_dataset = FeatureDataset(test_feature_vectors, test_labels)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=len(test_labels), shuffle=True)\n",
        "\n",
        "avg_classification_loss, avg_reconstruction_loss, avg_total_loss = model.training_step(test_data_loader, device)\n",
        "\n",
        "print(f\"Total Loss: {avg_total_loss}, Classification Loss: {avg_classification_loss}, Reconstruction Loss: {avg_reconstruction_loss}\")\n",
        "\n",
        "accuracy = model.calculate_accuracy(test_data_loader, device)\n",
        "print(f\"Accuracy: {accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcBDBR8LLNb9",
        "outputId": "fe6b7952-b40e-426e-f7cd-a159a0043417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Loss: 22861760512.0, Classification Loss: 8.051018714904785, Reconstruction Loss: 22861760512.0\n",
            "Accuracy: 37.69%\n"
          ]
        }
      ]
    }
  ]
}