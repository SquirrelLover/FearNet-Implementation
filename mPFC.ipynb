{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ/dt9eLwxJkTdIoFSWPqD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SquirrelLover/FearNet-Implementation/blob/main/mPFC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "NAa1Nl-k21SB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mini_batch = 450\n",
        "Dropout = 0.25\n",
        "Learning_Rate = 2e-3"
      ],
      "metadata": {
        "id": "0yTRBfMI1VgO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset (CIFAR-100)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # insert more transformations of data i.e. normalization\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR100(root='./data', train=True, transform = transform, download = True)\n",
        "test_data = torchvision.datasets.CIFAR100(root='./data', train=False, transform = transform, download = True)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = Mini_batch, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size = Mini_batch, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# view a couple of sample images to make sure they are loaded\n",
        "\n",
        "print(\"Train Loader Images:\")\n",
        "images, labels = next(iter(train_loader))\n",
        "images = images.cpu().detach().numpy()\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  plt.subplot(1, 4, i+1)\n",
        "  plt.imshow(np.transpose(images[i], (1, 2, 0)))\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"Image of class {labels[i].item()}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "gjJ6jJsagCvO",
        "outputId": "05404dd5-8d73-43a2-e595-ac75e1d19203"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train Loader Images:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FUxN0nB320I6"
      },
      "outputs": [],
      "source": [
        "# define mPFC (long term)\n",
        "\n",
        "class mPFC(nn.Module):\n",
        "  def __init__(self, input_dim=3072, autoencoder_hidden_dims = np.array([256]), classifier_dims = np.array([961, 525, 289, 100]), lambda_values=None, learning_rate=Learning_Rate):\n",
        "    super(mPFC, self).__init__()\n",
        "    # encoder\n",
        "    encoder_layers = []\n",
        "    current_dim = input_dim\n",
        "    for hidden_dim in autoencoder_hidden_dims:\n",
        "      print(hidden_dim)\n",
        "      encoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "      encoder_layers.append(nn.ELU())\n",
        "      current_dim = hidden_dim\n",
        "    self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "    # decoder\n",
        "    decoder_layers = []\n",
        "    hidden_dims_reversed = list(autoencoder_hidden_dims[::-1])\n",
        "    for hidden_dim in hidden_dims_reversed:\n",
        "      decoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "      decoder_layers.append(nn.ELU())\n",
        "      current_dim = hidden_dim\n",
        "    decoder_layers.append(nn.Linear(current_dim, input_dim))\n",
        "    decoder_layers.append(nn.ELU())\n",
        "    self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    #classifier\n",
        "    current_dim = autoencoder_hidden_dims[-1]\n",
        "    classifier_layers= []\n",
        "\n",
        "    if len(classifier_dims) != 1:\n",
        "      classifier_layers.append(nn.Linear(current_dim, classifier_dims[0]))\n",
        "      classifier_layers.append(nn.ELU())\n",
        "      #classifier_layers.append(nn.Dropout(p=Dropout))\n",
        "      current_dim = classifier_dims[0]\n",
        "\n",
        "      for i in range(1, len(classifier_dims)-1):\n",
        "        hidden_dim = classifier_dims[i]\n",
        "        classifier_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "        classifier_layers.append(nn.ELU())\n",
        "        #classifier_layers.append(nn.Dropout(p=Dropout))\n",
        "        current_dim = hidden_dim\n",
        "\n",
        "    classifier_layers.append(nn.Linear(current_dim, classifier_dims[len(classifier_dims)-1]))\n",
        "\n",
        "    self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    # lambda\n",
        "    self.lambda_values = torch.tensor(lambda_values if lambda_values else [1.0] * len(autoencoder_hidden_dims), dtype=torch.float32)\n",
        "\n",
        "    # optimizer\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "  def encoder_forward(self, x):\n",
        "    encoder_intermediates = [x]\n",
        "    for layer in self.encoder:\n",
        "      x = layer(x)\n",
        "      encoder_intermediates.append(x)\n",
        "    encoded = encoder_intermediates[-1]\n",
        "    return encoded, encoder_intermediates\n",
        "\n",
        "  def decoder_forward(self, x):\n",
        "    decoder_intermediates = [x]\n",
        "    for layer in self.decoder:\n",
        "      x = layer(x)\n",
        "      decoder_intermediates.append(x)\n",
        "    pseudo_img = decoder_intermediates[-1]\n",
        "    return pseudo_img, list(decoder_intermediates[::-1])\n",
        "\n",
        "  def classify(self, x):\n",
        "    class_logits = self.classifier(x)\n",
        "    return class_logits\n",
        "\n",
        "  def compute_loss(self, class_logits, targets, encoder_intermediates, decoder_intermediates):\n",
        "    # classification loss\n",
        "    classification_loss = nn.CrossEntropyLoss()(class_logits, targets)\n",
        "\n",
        "    # reconstruction loss with lambda weighting\n",
        "    reconstruction_loss = 0\n",
        "    for i in range(len(self.lambda_values)):\n",
        "      encoder_hidden = encoder_intermediates[i]\n",
        "      decoder_hidden = decoder_intermediates[i]\n",
        "      diff = encoder_hidden - decoder_hidden\n",
        "      squared_diff = diff.pow(2)\n",
        "      layer_loss = squared_diff.sum()\n",
        "      reconstruction_loss += self.lambda_values[i] * layer_loss\n",
        "\n",
        "    # total loss\n",
        "    total_loss = classification_loss + reconstruction_loss\n",
        "\n",
        "    return classification_loss, reconstruction_loss, total_loss\n",
        "\n",
        "  def training_step(self, data_loader, device):\n",
        "    self.train()\n",
        "\n",
        "    total_classification_loss = 0\n",
        "    total_reconstruction_loss = 0\n",
        "    total_total_loss = 0\n",
        "\n",
        "    for x, targets in data_loader:\n",
        "      x, targets = x.to(device), targets.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      encoded, encoder_intermediates = self.encoder_forward(x)\n",
        "      pseudo_img, decoder_intermediates = self.decoder_forward(encoded)\n",
        "      class_logits = self.classify(encoded)\n",
        "\n",
        "      # compute losses\n",
        "      classification_loss, reconstruction_loss, total_loss = self.compute_loss(\n",
        "        class_logits, targets, encoder_intermediates, decoder_intermediates\n",
        "      )\n",
        "\n",
        "      # zero gradients\n",
        "      self.optimizer.zero_grad()\n",
        "      for param in self.decoder.parameters():\n",
        "        param.grad = None\n",
        "      for param in self.encoder.parameters():\n",
        "        param.grad = None\n",
        "\n",
        "      # backward pass\n",
        "      classification_loss.backward(retain_graph=True)\n",
        "      classifier_grads = {name: param.grad.clone() for name, param in self.classifier.named_parameters()}\n",
        "      reconstruction_loss.backward(retain_graph=True)\n",
        "      decoder_grads = {name: param.grad.clone() for name, param in self.decoder.named_parameters()}\n",
        "      total_loss.backward()\n",
        "      for name, param in self.encoder.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = param.grad.data\n",
        "\n",
        "      # optimizer\n",
        "      encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      encoder_optimizer.step()\n",
        "      classifier_optimizer = optim.Adam(self.classifier.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      classifier_optimizer.step()\n",
        "      decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      decoder_optimizer.step()\n",
        "\n",
        "      # update parameters\n",
        "      for name, param in self.classifier.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = classifier_grads[name].data\n",
        "      for name, param in self.decoder.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = decoder_grads[name].data\n",
        "\n",
        "      # add losses to total\n",
        "      total_classification_loss += classification_loss.item()\n",
        "      total_reconstruction_loss += reconstruction_loss.item()\n",
        "      total_total_loss += total_loss.item()\n",
        "\n",
        "    # average loss\n",
        "    average_classification_loss = total_classification_loss / len(data_loader)\n",
        "    average_reconstruction_loss = total_reconstruction_loss / len(data_loader)\n",
        "    average_total_loss = total_total_loss / len(data_loader)\n",
        "\n",
        "    return average_classification_loss, average_reconstruction_loss, average_total_loss\n",
        "\n",
        "  def generate_statistics(self, list_feature_vectors, device):\n",
        "    self.eval()\n",
        "\n",
        "    latent_rep = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, targets in list_feature_vectors:\n",
        "        x, targets = x.to(device), targets.to(device)\n",
        "\n",
        "        # pass through encoder\n",
        "        mPFC_out, _ = self.encoder_forward(x)\n",
        "        latent_rep.append(mPFC_out.cpu())\n",
        "\n",
        "    # stack\n",
        "    latent_rep = torch.cat(latent_rep, dim=0)\n",
        "\n",
        "    # ux\n",
        "    ux_mPFC = latent_rep.mean(dim=0)\n",
        "\n",
        "    # covariance matrix\n",
        "    centered_mPFC_features = latent_rep - ux_mPFC\n",
        "    covariance_matrix_mPFC = torch.matmul(centered_mPFC_features.t(), centered_mPFC_features) / (latent_rep.size(0) - 1)\n",
        "\n",
        "    return ux_mPFC, covariance_matrix_mPFC\n",
        "\n",
        "  def pseudoimg_from_statistics(self, u, covar, count):\n",
        "    self.eval()\n",
        "\n",
        "    # sampling from a multivariate normal distribution\n",
        "    distribution = torch.distributions.MultivariateNormal(u, covar)\n",
        "    sampled_vectors = distribution.sample((count,)).to(device)\n",
        "\n",
        "    # pass through decoder\n",
        "    pseudo_images = []\n",
        "    with torch.no_grad():\n",
        "      for latent_vector in sampled_vectors:\n",
        "        pseudo_img, _ = self.decoder_forward(latent_vector)\n",
        "        pseudo_images.append(pseudo_img.cpu())\n",
        "\n",
        "    # stack\n",
        "    pseudo_images = torch.stack(pseudo_images, dim=0)\n",
        "\n",
        "    return pseudo_images\n",
        "\n",
        "  def calculate_accuracy(self, data_loader, device):\n",
        "    self.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, targets in data_loader:\n",
        "        x, targets = x.to(device), targets.to(device)\n",
        "        encoded, _ = self.encoder_forward(x)\n",
        "        class_logits = self.classify(encoded)\n",
        "        _, predicted = torch.max(class_logits.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "        return feature, label\n",
        "\n",
        "model = mPFC(learning_rate=1e-3)\n",
        "model = model.to(device)\n",
        "\n",
        "feature_vectors = torch.flatten(torch.from_numpy(images), start_dim=1)\n",
        "dataset = FeatureDataset(feature_vectors, labels)\n",
        "data_loader = DataLoader(dataset, batch_size=Mini_batch, shuffle=True)\n",
        "\n",
        "for epoch in range(100):\n",
        "    data_loader = DataLoader(dataset, batch_size=Mini_batch, shuffle=True)\n",
        "\n",
        "    avg_classification_loss, avg_reconstruction_loss, avg_total_loss = model.training_step(data_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Total Loss: {avg_total_loss}, Classification Loss: {avg_classification_loss}, Reconstruction Loss: {avg_reconstruction_loss}\")\n",
        "    for param_group in model.optimizer.param_groups:\n",
        "        print(\"Learning rate:\", param_group['lr'])\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            print(f\"{name} gradient norm: {param.grad.norm().item()}\")\n",
        "\n",
        "    accuracy = model.calculate_accuracy(data_loader, device)\n",
        "    print(f\"Epoch {epoch+1}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzxGXR78gedJ",
        "outputId": "d959a867-4012-4f38-de04-f7e7ab4401c7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256\n",
            "Epoch 1, Total Loss: 434501.84375, Classification Loss: 4.603724479675293, Reconstruction Loss: 434497.25\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 405622.71875\n",
            "encoder.0.bias gradient norm: 14109.0029296875\n",
            "decoder.0.weight gradient norm: 64699.3984375\n",
            "decoder.0.bias gradient norm: 14289.9345703125\n",
            "decoder.2.weight gradient norm: 60413.3125\n",
            "decoder.2.bias gradient norm: 23314.29296875\n",
            "classifier.0.weight gradient norm: 0.042778130620718\n",
            "classifier.0.bias gradient norm: 0.008290215395390987\n",
            "classifier.2.weight gradient norm: 0.0852585881948471\n",
            "classifier.2.bias gradient norm: 0.015290752984583378\n",
            "classifier.4.weight gradient norm: 0.06135959178209305\n",
            "classifier.4.bias gradient norm: 0.026706796139478683\n",
            "classifier.6.weight gradient norm: 0.0468364916741848\n",
            "classifier.6.bias gradient norm: 0.04707425832748413\n",
            "Epoch 1, Accuracy: 0.2222222222222222%\n",
            "Epoch 2, Total Loss: 490430.03125, Classification Loss: 4.610817909240723, Reconstruction Loss: 490425.40625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 713934.1875\n",
            "encoder.0.bias gradient norm: 25432.525390625\n",
            "decoder.0.weight gradient norm: 401011.75\n",
            "decoder.0.bias gradient norm: 21773.37890625\n",
            "decoder.2.weight gradient norm: 217285.3125\n",
            "decoder.2.bias gradient norm: 20506.126953125\n",
            "classifier.0.weight gradient norm: 0.21555013954639435\n",
            "classifier.0.bias gradient norm: 0.010895499028265476\n",
            "classifier.2.weight gradient norm: 0.30918455123901367\n",
            "classifier.2.bias gradient norm: 0.01670774444937706\n",
            "classifier.4.weight gradient norm: 0.19891542196273804\n",
            "classifier.4.bias gradient norm: 0.02809765376150608\n",
            "classifier.6.weight gradient norm: 0.149135559797287\n",
            "classifier.6.bias gradient norm: 0.05055123195052147\n",
            "Epoch 2, Accuracy: 2.0%\n",
            "Epoch 3, Total Loss: 386792.6875, Classification Loss: 4.515661716461182, Reconstruction Loss: 386788.15625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 735469.4375\n",
            "encoder.0.bias gradient norm: 25878.45703125\n",
            "decoder.0.weight gradient norm: 331461.28125\n",
            "decoder.0.bias gradient norm: 19590.85546875\n",
            "decoder.2.weight gradient norm: 187465.421875\n",
            "decoder.2.bias gradient norm: 19024.921875\n",
            "classifier.0.weight gradient norm: 0.06076021492481232\n",
            "classifier.0.bias gradient norm: 0.005080957897007465\n",
            "classifier.2.weight gradient norm: 0.12016040831804276\n",
            "classifier.2.bias gradient norm: 0.009071355685591698\n",
            "classifier.4.weight gradient norm: 0.13020716607570648\n",
            "classifier.4.bias gradient norm: 0.016131991520524025\n",
            "classifier.6.weight gradient norm: 0.18128959834575653\n",
            "classifier.6.bias gradient norm: 0.03319265693426132\n",
            "Epoch 3, Accuracy: 2.4444444444444446%\n",
            "Epoch 4, Total Loss: 369864.9375, Classification Loss: 4.483583927154541, Reconstruction Loss: 369860.46875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 822892.5\n",
            "encoder.0.bias gradient norm: 28832.171875\n",
            "decoder.0.weight gradient norm: 426051.75\n",
            "decoder.0.bias gradient norm: 21292.08203125\n",
            "decoder.2.weight gradient norm: 201966.28125\n",
            "decoder.2.bias gradient norm: 18336.8203125\n",
            "classifier.0.weight gradient norm: 0.08656223118305206\n",
            "classifier.0.bias gradient norm: 0.004337646067142487\n",
            "classifier.2.weight gradient norm: 0.14868474006652832\n",
            "classifier.2.bias gradient norm: 0.007907477207481861\n",
            "classifier.4.weight gradient norm: 0.15883171558380127\n",
            "classifier.4.bias gradient norm: 0.014390441589057446\n",
            "classifier.6.weight gradient norm: 0.18286389112472534\n",
            "classifier.6.bias gradient norm: 0.02541731297969818\n",
            "Epoch 4, Accuracy: 2.888888888888889%\n",
            "Epoch 5, Total Loss: 305261.65625, Classification Loss: 4.476585388183594, Reconstruction Loss: 305257.1875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 893253.25\n",
            "encoder.0.bias gradient norm: 31410.041015625\n",
            "decoder.0.weight gradient norm: 461219.1875\n",
            "decoder.0.bias gradient norm: 21824.435546875\n",
            "decoder.2.weight gradient norm: 219751.984375\n",
            "decoder.2.bias gradient norm: 17038.50390625\n",
            "classifier.0.weight gradient norm: 0.1707998365163803\n",
            "classifier.0.bias gradient norm: 0.00924694910645485\n",
            "classifier.2.weight gradient norm: 0.26800736784935\n",
            "classifier.2.bias gradient norm: 0.013487002812325954\n",
            "classifier.4.weight gradient norm: 0.3218751847743988\n",
            "classifier.4.bias gradient norm: 0.023473652079701424\n",
            "classifier.6.weight gradient norm: 0.37785959243774414\n",
            "classifier.6.bias gradient norm: 0.03982976824045181\n",
            "Epoch 5, Accuracy: 1.7777777777777777%\n",
            "Epoch 6, Total Loss: 299372.78125, Classification Loss: 4.446268558502197, Reconstruction Loss: 299368.34375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 870215.1875\n",
            "encoder.0.bias gradient norm: 30113.087890625\n",
            "decoder.0.weight gradient norm: 472560.34375\n",
            "decoder.0.bias gradient norm: 22721.044921875\n",
            "decoder.2.weight gradient norm: 176363.921875\n",
            "decoder.2.bias gradient norm: 17096.451171875\n",
            "classifier.0.weight gradient norm: 0.12249570339918137\n",
            "classifier.0.bias gradient norm: 0.008611193858087063\n",
            "classifier.2.weight gradient norm: 0.17193694412708282\n",
            "classifier.2.bias gradient norm: 0.011745975352823734\n",
            "classifier.4.weight gradient norm: 0.17617200314998627\n",
            "classifier.4.bias gradient norm: 0.01906491443514824\n",
            "classifier.6.weight gradient norm: 0.19319187104701996\n",
            "classifier.6.bias gradient norm: 0.03021485172212124\n",
            "Epoch 6, Accuracy: 2.0%\n",
            "Epoch 7, Total Loss: 244333.96875, Classification Loss: 4.434204578399658, Reconstruction Loss: 244329.53125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 968665.0\n",
            "encoder.0.bias gradient norm: 34349.875\n",
            "decoder.0.weight gradient norm: 600844.0\n",
            "decoder.0.bias gradient norm: 25381.888671875\n",
            "decoder.2.weight gradient norm: 227652.046875\n",
            "decoder.2.bias gradient norm: 15907.443359375\n",
            "classifier.0.weight gradient norm: 0.2978024482727051\n",
            "classifier.0.bias gradient norm: 0.009775659069418907\n",
            "classifier.2.weight gradient norm: 0.3370220959186554\n",
            "classifier.2.bias gradient norm: 0.013808252289891243\n",
            "classifier.4.weight gradient norm: 0.3147980868816376\n",
            "classifier.4.bias gradient norm: 0.02172587811946869\n",
            "classifier.6.weight gradient norm: 0.33908551931381226\n",
            "classifier.6.bias gradient norm: 0.03472105413675308\n",
            "Epoch 7, Accuracy: 3.7777777777777777%\n",
            "Epoch 8, Total Loss: 254231.65625, Classification Loss: 4.420835494995117, Reconstruction Loss: 254227.234375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 909428.1875\n",
            "encoder.0.bias gradient norm: 31200.513671875\n",
            "decoder.0.weight gradient norm: 531241.625\n",
            "decoder.0.bias gradient norm: 24461.388671875\n",
            "decoder.2.weight gradient norm: 151219.09375\n",
            "decoder.2.bias gradient norm: 15948.3427734375\n",
            "classifier.0.weight gradient norm: 0.3565262258052826\n",
            "classifier.0.bias gradient norm: 0.022079886868596077\n",
            "classifier.2.weight gradient norm: 0.3658475875854492\n",
            "classifier.2.bias gradient norm: 0.022573592141270638\n",
            "classifier.4.weight gradient norm: 0.3362697660923004\n",
            "classifier.4.bias gradient norm: 0.03173257037997246\n",
            "classifier.6.weight gradient norm: 0.3782389163970947\n",
            "classifier.6.bias gradient norm: 0.0494726188480854\n",
            "Epoch 8, Accuracy: 2.888888888888889%\n",
            "Epoch 9, Total Loss: 245197.921875, Classification Loss: 4.400562763214111, Reconstruction Loss: 245193.515625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1223902.25\n",
            "encoder.0.bias gradient norm: 43330.3984375\n",
            "decoder.0.weight gradient norm: 873351.0\n",
            "decoder.0.bias gradient norm: 32819.59375\n",
            "decoder.2.weight gradient norm: 262136.28125\n",
            "decoder.2.bias gradient norm: 16858.3828125\n",
            "classifier.0.weight gradient norm: 0.3858674168586731\n",
            "classifier.0.bias gradient norm: 0.015051394701004028\n",
            "classifier.2.weight gradient norm: 0.3829177916049957\n",
            "classifier.2.bias gradient norm: 0.017595982179045677\n",
            "classifier.4.weight gradient norm: 0.3416890799999237\n",
            "classifier.4.bias gradient norm: 0.026016265153884888\n",
            "classifier.6.weight gradient norm: 0.3741477131843567\n",
            "classifier.6.bias gradient norm: 0.04349840432405472\n",
            "Epoch 9, Accuracy: 3.3333333333333335%\n",
            "Epoch 10, Total Loss: 229604.59375, Classification Loss: 4.519381046295166, Reconstruction Loss: 229600.078125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 933682.9375\n",
            "encoder.0.bias gradient norm: 31837.228515625\n",
            "decoder.0.weight gradient norm: 604435.5\n",
            "decoder.0.bias gradient norm: 27815.99609375\n",
            "decoder.2.weight gradient norm: 136315.828125\n",
            "decoder.2.bias gradient norm: 15361.5830078125\n",
            "classifier.0.weight gradient norm: 1.0751255750656128\n",
            "classifier.0.bias gradient norm: 0.04089924693107605\n",
            "classifier.2.weight gradient norm: 0.8243021965026855\n",
            "classifier.2.bias gradient norm: 0.037976909428834915\n",
            "classifier.4.weight gradient norm: 0.4422854483127594\n",
            "classifier.4.bias gradient norm: 0.04101191461086273\n",
            "classifier.6.weight gradient norm: 0.37787064909935\n",
            "classifier.6.bias gradient norm: 0.05342419818043709\n",
            "Epoch 10, Accuracy: 1.7777777777777777%\n",
            "Epoch 11, Total Loss: 253489.109375, Classification Loss: 5.536653995513916, Reconstruction Loss: 253483.578125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1368935.375\n",
            "encoder.0.bias gradient norm: 48466.73046875\n",
            "decoder.0.weight gradient norm: 1069646.625\n",
            "decoder.0.bias gradient norm: 37751.59375\n",
            "decoder.2.weight gradient norm: 284931.65625\n",
            "decoder.2.bias gradient norm: 17684.615234375\n",
            "classifier.0.weight gradient norm: 3.8771438598632812\n",
            "classifier.0.bias gradient norm: 0.14638520777225494\n",
            "classifier.2.weight gradient norm: 3.5368454456329346\n",
            "classifier.2.bias gradient norm: 0.13770897686481476\n",
            "classifier.4.weight gradient norm: 4.503305912017822\n",
            "classifier.4.bias gradient norm: 0.18894830346107483\n",
            "classifier.6.weight gradient norm: 7.935147285461426\n",
            "classifier.6.bias gradient norm: 0.36651158332824707\n",
            "Epoch 11, Accuracy: 4.666666666666667%\n",
            "Epoch 12, Total Loss: 238188.984375, Classification Loss: 4.401029586791992, Reconstruction Loss: 238184.578125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1010296.75\n",
            "encoder.0.bias gradient norm: 34535.94921875\n",
            "decoder.0.weight gradient norm: 701342.625\n",
            "decoder.0.bias gradient norm: 29140.876953125\n",
            "decoder.2.weight gradient norm: 149234.296875\n",
            "decoder.2.bias gradient norm: 15516.2470703125\n",
            "classifier.0.weight gradient norm: 0.38012373447418213\n",
            "classifier.0.bias gradient norm: 0.022519363090395927\n",
            "classifier.2.weight gradient norm: 0.3641819953918457\n",
            "classifier.2.bias gradient norm: 0.02182099223136902\n",
            "classifier.4.weight gradient norm: 0.27608177065849304\n",
            "classifier.4.bias gradient norm: 0.026088330894708633\n",
            "classifier.6.weight gradient norm: 0.2619168162345886\n",
            "classifier.6.bias gradient norm: 0.03805362433195114\n",
            "Epoch 12, Accuracy: 2.2222222222222223%\n",
            "Epoch 13, Total Loss: 236744.15625, Classification Loss: 4.691092014312744, Reconstruction Loss: 236739.46875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1248107.75\n",
            "encoder.0.bias gradient norm: 44360.328125\n",
            "decoder.0.weight gradient norm: 1014050.0625\n",
            "decoder.0.bias gradient norm: 34270.09375\n",
            "decoder.2.weight gradient norm: 269254.5\n",
            "decoder.2.bias gradient norm: 16933.576171875\n",
            "classifier.0.weight gradient norm: 1.7631256580352783\n",
            "classifier.0.bias gradient norm: 0.06379881501197815\n",
            "classifier.2.weight gradient norm: 1.403814435005188\n",
            "classifier.2.bias gradient norm: 0.05645639821887016\n",
            "classifier.4.weight gradient norm: 1.360605001449585\n",
            "classifier.4.bias gradient norm: 0.06763017922639847\n",
            "classifier.6.weight gradient norm: 1.7926744222640991\n",
            "classifier.6.bias gradient norm: 0.1149585172533989\n",
            "Epoch 13, Accuracy: 5.777777777777778%\n",
            "Epoch 14, Total Loss: 255124.140625, Classification Loss: 4.363589286804199, Reconstruction Loss: 255119.78125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1128014.625\n",
            "encoder.0.bias gradient norm: 38726.765625\n",
            "decoder.0.weight gradient norm: 747432.25\n",
            "decoder.0.bias gradient norm: 28938.951171875\n",
            "decoder.2.weight gradient norm: 169222.25\n",
            "decoder.2.bias gradient norm: 15887.6904296875\n",
            "classifier.0.weight gradient norm: 0.7806239128112793\n",
            "classifier.0.bias gradient norm: 0.025486860424280167\n",
            "classifier.2.weight gradient norm: 0.5800961256027222\n",
            "classifier.2.bias gradient norm: 0.02302786335349083\n",
            "classifier.4.weight gradient norm: 0.37540486454963684\n",
            "classifier.4.bias gradient norm: 0.02588469348847866\n",
            "classifier.6.weight gradient norm: 0.3383016288280487\n",
            "classifier.6.bias gradient norm: 0.03603123873472214\n",
            "Epoch 14, Accuracy: 2.0%\n",
            "Epoch 15, Total Loss: 249235.453125, Classification Loss: 4.779332160949707, Reconstruction Loss: 249230.671875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1325820.25\n",
            "encoder.0.bias gradient norm: 47089.0\n",
            "decoder.0.weight gradient norm: 1067632.25\n",
            "decoder.0.bias gradient norm: 34855.70703125\n",
            "decoder.2.weight gradient norm: 275761.71875\n",
            "decoder.2.bias gradient norm: 17505.71875\n",
            "classifier.0.weight gradient norm: 3.1117191314697266\n",
            "classifier.0.bias gradient norm: 0.11575387418270111\n",
            "classifier.2.weight gradient norm: 1.9801161289215088\n",
            "classifier.2.bias gradient norm: 0.08659927546977997\n",
            "classifier.4.weight gradient norm: 1.6036044359207153\n",
            "classifier.4.bias gradient norm: 0.08736693114042282\n",
            "classifier.6.weight gradient norm: 2.22446608543396\n",
            "classifier.6.bias gradient norm: 0.14082692563533783\n",
            "Epoch 15, Accuracy: 4.666666666666667%\n",
            "Epoch 16, Total Loss: 261341.328125, Classification Loss: 4.419930934906006, Reconstruction Loss: 261336.90625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1095260.375\n",
            "encoder.0.bias gradient norm: 37686.1171875\n",
            "decoder.0.weight gradient norm: 810916.0\n",
            "decoder.0.bias gradient norm: 29027.3671875\n",
            "decoder.2.weight gradient norm: 173555.984375\n",
            "decoder.2.bias gradient norm: 16185.791015625\n",
            "classifier.0.weight gradient norm: 1.1206170320510864\n",
            "classifier.0.bias gradient norm: 0.03223061561584473\n",
            "classifier.2.weight gradient norm: 0.7858979105949402\n",
            "classifier.2.bias gradient norm: 0.029993049800395966\n",
            "classifier.4.weight gradient norm: 0.4530235826969147\n",
            "classifier.4.bias gradient norm: 0.033848993480205536\n",
            "classifier.6.weight gradient norm: 0.3434029817581177\n",
            "classifier.6.bias gradient norm: 0.04409899562597275\n",
            "Epoch 16, Accuracy: 1.7777777777777777%\n",
            "Epoch 17, Total Loss: 227731.828125, Classification Loss: 4.770345211029053, Reconstruction Loss: 227727.0625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1152289.0\n",
            "encoder.0.bias gradient norm: 40833.17578125\n",
            "decoder.0.weight gradient norm: 990357.125\n",
            "decoder.0.bias gradient norm: 31717.423828125\n",
            "decoder.2.weight gradient norm: 244571.96875\n",
            "decoder.2.bias gradient norm: 16337.7919921875\n",
            "classifier.0.weight gradient norm: 2.77939510345459\n",
            "classifier.0.bias gradient norm: 0.10368549823760986\n",
            "classifier.2.weight gradient norm: 1.814717173576355\n",
            "classifier.2.bias gradient norm: 0.07842905074357986\n",
            "classifier.4.weight gradient norm: 1.4412071704864502\n",
            "classifier.4.bias gradient norm: 0.08155635744333267\n",
            "classifier.6.weight gradient norm: 1.8645412921905518\n",
            "classifier.6.bias gradient norm: 0.12663133442401886\n",
            "Epoch 17, Accuracy: 2.0%\n",
            "Epoch 18, Total Loss: 228763.109375, Classification Loss: 4.582118034362793, Reconstruction Loss: 228758.53125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 897081.125\n",
            "encoder.0.bias gradient norm: 30775.7578125\n",
            "decoder.0.weight gradient norm: 723790.75\n",
            "decoder.0.bias gradient norm: 25653.845703125\n",
            "decoder.2.weight gradient norm: 155599.640625\n",
            "decoder.2.bias gradient norm: 15159.9033203125\n",
            "classifier.0.weight gradient norm: 2.157702684402466\n",
            "classifier.0.bias gradient norm: 0.07068751752376556\n",
            "classifier.2.weight gradient norm: 1.3613240718841553\n",
            "classifier.2.bias gradient norm: 0.05498550087213516\n",
            "classifier.4.weight gradient norm: 0.7942376136779785\n",
            "classifier.4.bias gradient norm: 0.05390061438083649\n",
            "classifier.6.weight gradient norm: 0.7937988638877869\n",
            "classifier.6.bias gradient norm: 0.07649395614862442\n",
            "Epoch 18, Accuracy: 2.6666666666666665%\n",
            "Epoch 19, Total Loss: 250458.890625, Classification Loss: 4.599693775177002, Reconstruction Loss: 250454.296875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1292774.5\n",
            "encoder.0.bias gradient norm: 45776.53515625\n",
            "decoder.0.weight gradient norm: 1089005.5\n",
            "decoder.0.bias gradient norm: 34820.1640625\n",
            "decoder.2.weight gradient norm: 271905.15625\n",
            "decoder.2.bias gradient norm: 17536.30859375\n",
            "classifier.0.weight gradient norm: 1.6329777240753174\n",
            "classifier.0.bias gradient norm: 0.06307726353406906\n",
            "classifier.2.weight gradient norm: 1.09845769405365\n",
            "classifier.2.bias gradient norm: 0.048705536872148514\n",
            "classifier.4.weight gradient norm: 0.8997443914413452\n",
            "classifier.4.bias gradient norm: 0.054232195019721985\n",
            "classifier.6.weight gradient norm: 1.0376896858215332\n",
            "classifier.6.bias gradient norm: 0.08622103929519653\n",
            "Epoch 19, Accuracy: 2.6666666666666665%\n",
            "Epoch 20, Total Loss: 237686.96875, Classification Loss: 4.384099006652832, Reconstruction Loss: 237682.578125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1007095.125\n",
            "encoder.0.bias gradient norm: 34551.94140625\n",
            "decoder.0.weight gradient norm: 768800.3125\n",
            "decoder.0.bias gradient norm: 27423.908203125\n",
            "decoder.2.weight gradient norm: 160933.71875\n",
            "decoder.2.bias gradient norm: 15459.125\n",
            "classifier.0.weight gradient norm: 0.7866336703300476\n",
            "classifier.0.bias gradient norm: 0.03032250888645649\n",
            "classifier.2.weight gradient norm: 0.5643247365951538\n",
            "classifier.2.bias gradient norm: 0.02603195048868656\n",
            "classifier.4.weight gradient norm: 0.3527257442474365\n",
            "classifier.4.bias gradient norm: 0.02892843633890152\n",
            "classifier.6.weight gradient norm: 0.31103384494781494\n",
            "classifier.6.bias gradient norm: 0.041732609272003174\n",
            "Epoch 20, Accuracy: 2.0%\n",
            "Epoch 21, Total Loss: 219610.6875, Classification Loss: 4.445211410522461, Reconstruction Loss: 219606.25\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1130999.625\n",
            "encoder.0.bias gradient norm: 40273.0234375\n",
            "decoder.0.weight gradient norm: 961439.125\n",
            "decoder.0.bias gradient norm: 30393.32421875\n",
            "decoder.2.weight gradient norm: 246276.375\n",
            "decoder.2.bias gradient norm: 16008.912109375\n",
            "classifier.0.weight gradient norm: 1.322489857673645\n",
            "classifier.0.bias gradient norm: 0.04074415564537048\n",
            "classifier.2.weight gradient norm: 0.9461989402770996\n",
            "classifier.2.bias gradient norm: 0.03519000485539436\n",
            "classifier.4.weight gradient norm: 0.8156532645225525\n",
            "classifier.4.bias gradient norm: 0.04470789432525635\n",
            "classifier.6.weight gradient norm: 1.0344537496566772\n",
            "classifier.6.bias gradient norm: 0.08174819499254227\n",
            "Epoch 21, Accuracy: 1.7777777777777777%\n",
            "Epoch 22, Total Loss: 283172.84375, Classification Loss: 4.709982872009277, Reconstruction Loss: 283168.125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1204225.25\n",
            "encoder.0.bias gradient norm: 41551.7578125\n",
            "decoder.0.weight gradient norm: 941303.0\n",
            "decoder.0.bias gradient norm: 31798.44921875\n",
            "decoder.2.weight gradient norm: 167990.59375\n",
            "decoder.2.bias gradient norm: 17037.5234375\n",
            "classifier.0.weight gradient norm: 3.0964932441711426\n",
            "classifier.0.bias gradient norm: 0.12276183068752289\n",
            "classifier.2.weight gradient norm: 1.7423189878463745\n",
            "classifier.2.bias gradient norm: 0.08068965375423431\n",
            "classifier.4.weight gradient norm: 1.2781267166137695\n",
            "classifier.4.bias gradient norm: 0.07871727645397186\n",
            "classifier.6.weight gradient norm: 1.6153135299682617\n",
            "classifier.6.bias gradient norm: 0.12086817622184753\n",
            "Epoch 22, Accuracy: 1.5555555555555556%\n",
            "Epoch 23, Total Loss: 259485.34375, Classification Loss: 4.7497053146362305, Reconstruction Loss: 259480.59375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1333233.625\n",
            "encoder.0.bias gradient norm: 47522.640625\n",
            "decoder.0.weight gradient norm: 1267806.875\n",
            "decoder.0.bias gradient norm: 35943.3984375\n",
            "decoder.2.weight gradient norm: 299429.03125\n",
            "decoder.2.bias gradient norm: 18118.041015625\n",
            "classifier.0.weight gradient norm: 3.1941373348236084\n",
            "classifier.0.bias gradient norm: 0.0803847461938858\n",
            "classifier.2.weight gradient norm: 1.9146392345428467\n",
            "classifier.2.bias gradient norm: 0.06253663450479507\n",
            "classifier.4.weight gradient norm: 1.3503568172454834\n",
            "classifier.4.bias gradient norm: 0.06785309314727783\n",
            "classifier.6.weight gradient norm: 1.6100603342056274\n",
            "classifier.6.bias gradient norm: 0.11700936406850815\n",
            "Epoch 23, Accuracy: 3.111111111111111%\n",
            "Epoch 24, Total Loss: 258823.578125, Classification Loss: 4.4496612548828125, Reconstruction Loss: 258819.125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1110353.875\n",
            "encoder.0.bias gradient norm: 38112.21484375\n",
            "decoder.0.weight gradient norm: 941171.0625\n",
            "decoder.0.bias gradient norm: 30341.25390625\n",
            "decoder.2.weight gradient norm: 151382.171875\n",
            "decoder.2.bias gradient norm: 16310.828125\n",
            "classifier.0.weight gradient norm: 1.3994722366333008\n",
            "classifier.0.bias gradient norm: 0.05953488126397133\n",
            "classifier.2.weight gradient norm: 0.8295563459396362\n",
            "classifier.2.bias gradient norm: 0.04125566408038139\n",
            "classifier.4.weight gradient norm: 0.5956389307975769\n",
            "classifier.4.bias gradient norm: 0.043391164392232895\n",
            "classifier.6.weight gradient norm: 0.5691171288490295\n",
            "classifier.6.bias gradient norm: 0.0647391602396965\n",
            "Epoch 24, Accuracy: 1.7777777777777777%\n",
            "Epoch 25, Total Loss: 214824.6875, Classification Loss: 4.471568584442139, Reconstruction Loss: 214820.21875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1075872.0\n",
            "encoder.0.bias gradient norm: 38667.72265625\n",
            "decoder.0.weight gradient norm: 1043443.25\n",
            "decoder.0.bias gradient norm: 30246.068359375\n",
            "decoder.2.weight gradient norm: 255018.34375\n",
            "decoder.2.bias gradient norm: 15886.2392578125\n",
            "classifier.0.weight gradient norm: 1.4687168598175049\n",
            "classifier.0.bias gradient norm: 0.03386502340435982\n",
            "classifier.2.weight gradient norm: 0.9321250915527344\n",
            "classifier.2.bias gradient norm: 0.02898707240819931\n",
            "classifier.4.weight gradient norm: 0.779263973236084\n",
            "classifier.4.bias gradient norm: 0.03706991299986839\n",
            "classifier.6.weight gradient norm: 0.7298684120178223\n",
            "classifier.6.bias gradient norm: 0.059197377413511276\n",
            "Epoch 25, Accuracy: 1.7777777777777777%\n",
            "Epoch 26, Total Loss: 278431.96875, Classification Loss: 4.525050640106201, Reconstruction Loss: 278427.4375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1110279.625\n",
            "encoder.0.bias gradient norm: 38256.0390625\n",
            "decoder.0.weight gradient norm: 937403.0625\n",
            "decoder.0.bias gradient norm: 31017.583984375\n",
            "decoder.2.weight gradient norm: 164844.046875\n",
            "decoder.2.bias gradient norm: 16715.388671875\n",
            "classifier.0.weight gradient norm: 1.786627173423767\n",
            "classifier.0.bias gradient norm: 0.06998942047357559\n",
            "classifier.2.weight gradient norm: 1.1693121194839478\n",
            "classifier.2.bias gradient norm: 0.05172363668680191\n",
            "classifier.4.weight gradient norm: 1.166951298713684\n",
            "classifier.4.bias gradient norm: 0.06242337450385094\n",
            "classifier.6.weight gradient norm: 1.6380318403244019\n",
            "classifier.6.bias gradient norm: 0.1153545156121254\n",
            "Epoch 26, Accuracy: 2.4444444444444446%\n",
            "Epoch 27, Total Loss: 210416.328125, Classification Loss: 4.43947172164917, Reconstruction Loss: 210411.890625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1057608.25\n",
            "encoder.0.bias gradient norm: 37547.6953125\n",
            "decoder.0.weight gradient norm: 1011347.3125\n",
            "decoder.0.bias gradient norm: 29537.92578125\n",
            "decoder.2.weight gradient norm: 240072.640625\n",
            "decoder.2.bias gradient norm: 15550.3349609375\n",
            "classifier.0.weight gradient norm: 1.4379699230194092\n",
            "classifier.0.bias gradient norm: 0.034294672310352325\n",
            "classifier.2.weight gradient norm: 0.9294968843460083\n",
            "classifier.2.bias gradient norm: 0.03053239919245243\n",
            "classifier.4.weight gradient norm: 0.7573497295379639\n",
            "classifier.4.bias gradient norm: 0.038213711231946945\n",
            "classifier.6.weight gradient norm: 0.6110667586326599\n",
            "classifier.6.bias gradient norm: 0.0553419291973114\n",
            "Epoch 27, Accuracy: 1.7777777777777777%\n",
            "Epoch 28, Total Loss: 240841.4375, Classification Loss: 4.548812389373779, Reconstruction Loss: 240836.890625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 966683.0\n",
            "encoder.0.bias gradient norm: 33159.53125\n",
            "decoder.0.weight gradient norm: 828151.75\n",
            "decoder.0.bias gradient norm: 28745.20703125\n",
            "decoder.2.weight gradient norm: 164777.078125\n",
            "decoder.2.bias gradient norm: 15544.10546875\n",
            "classifier.0.weight gradient norm: 1.8956382274627686\n",
            "classifier.0.bias gradient norm: 0.08153647184371948\n",
            "classifier.2.weight gradient norm: 1.129752278327942\n",
            "classifier.2.bias gradient norm: 0.055405281484127045\n",
            "classifier.4.weight gradient norm: 0.9426048398017883\n",
            "classifier.4.bias gradient norm: 0.05891422927379608\n",
            "classifier.6.weight gradient norm: 0.9891169667243958\n",
            "classifier.6.bias gradient norm: 0.08608254790306091\n",
            "Epoch 28, Accuracy: 2.4444444444444446%\n",
            "Epoch 29, Total Loss: 233241.34375, Classification Loss: 4.512706279754639, Reconstruction Loss: 233236.828125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1153378.125\n",
            "encoder.0.bias gradient norm: 41123.609375\n",
            "decoder.0.weight gradient norm: 1094857.5\n",
            "decoder.0.bias gradient norm: 33648.9453125\n",
            "decoder.2.weight gradient norm: 249445.53125\n",
            "decoder.2.bias gradient norm: 16753.17578125\n",
            "classifier.0.weight gradient norm: 2.8358516693115234\n",
            "classifier.0.bias gradient norm: 0.0820428803563118\n",
            "classifier.2.weight gradient norm: 1.535730004310608\n",
            "classifier.2.bias gradient norm: 0.05679955706000328\n",
            "classifier.4.weight gradient norm: 1.0614975690841675\n",
            "classifier.4.bias gradient norm: 0.05817374959588051\n",
            "classifier.6.weight gradient norm: 1.0245356559753418\n",
            "classifier.6.bias gradient norm: 0.08462219685316086\n",
            "Epoch 29, Accuracy: 4.0%\n",
            "Epoch 30, Total Loss: 243883.984375, Classification Loss: 4.474529266357422, Reconstruction Loss: 243879.515625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1037907.25\n",
            "encoder.0.bias gradient norm: 35554.359375\n",
            "decoder.0.weight gradient norm: 876614.0\n",
            "decoder.0.bias gradient norm: 30376.29296875\n",
            "decoder.2.weight gradient norm: 162674.5\n",
            "decoder.2.bias gradient norm: 15712.41796875\n",
            "classifier.0.weight gradient norm: 1.8683953285217285\n",
            "classifier.0.bias gradient norm: 0.07730285078287125\n",
            "classifier.2.weight gradient norm: 1.0371369123458862\n",
            "classifier.2.bias gradient norm: 0.04920940473675728\n",
            "classifier.4.weight gradient norm: 0.7655205130577087\n",
            "classifier.4.bias gradient norm: 0.04965377226471901\n",
            "classifier.6.weight gradient norm: 0.847723662853241\n",
            "classifier.6.bias gradient norm: 0.07735320180654526\n",
            "Epoch 30, Accuracy: 6.222222222222222%\n",
            "Epoch 31, Total Loss: 262571.96875, Classification Loss: 4.333741188049316, Reconstruction Loss: 262567.625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1301695.5\n",
            "encoder.0.bias gradient norm: 46256.26953125\n",
            "decoder.0.weight gradient norm: 1155264.5\n",
            "decoder.0.bias gradient norm: 35203.296875\n",
            "decoder.2.weight gradient norm: 273733.375\n",
            "decoder.2.bias gradient norm: 18019.177734375\n",
            "classifier.0.weight gradient norm: 1.4902423620224\n",
            "classifier.0.bias gradient norm: 0.04828836768865585\n",
            "classifier.2.weight gradient norm: 0.7939106822013855\n",
            "classifier.2.bias gradient norm: 0.032593049108982086\n",
            "classifier.4.weight gradient norm: 0.5510745644569397\n",
            "classifier.4.bias gradient norm: 0.03516823425889015\n",
            "classifier.6.weight gradient norm: 0.48124873638153076\n",
            "classifier.6.bias gradient norm: 0.05019867792725563\n",
            "Epoch 31, Accuracy: 2.6666666666666665%\n",
            "Epoch 32, Total Loss: 239814.84375, Classification Loss: 4.461178779602051, Reconstruction Loss: 239810.375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1012292.25\n",
            "encoder.0.bias gradient norm: 34917.9921875\n",
            "decoder.0.weight gradient norm: 815635.875\n",
            "decoder.0.bias gradient norm: 27962.423828125\n",
            "decoder.2.weight gradient norm: 160376.625\n",
            "decoder.2.bias gradient norm: 15580.8291015625\n",
            "classifier.0.weight gradient norm: 2.0146422386169434\n",
            "classifier.0.bias gradient norm: 0.07250763475894928\n",
            "classifier.2.weight gradient norm: 1.1439834833145142\n",
            "classifier.2.bias gradient norm: 0.04768526181578636\n",
            "classifier.4.weight gradient norm: 0.84441739320755\n",
            "classifier.4.bias gradient norm: 0.05367925763130188\n",
            "classifier.6.weight gradient norm: 1.0788875818252563\n",
            "classifier.6.bias gradient norm: 0.09237028658390045\n",
            "Epoch 32, Accuracy: 4.222222222222222%\n",
            "Epoch 33, Total Loss: 228539.25, Classification Loss: 4.373612403869629, Reconstruction Loss: 228534.875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1207414.625\n",
            "encoder.0.bias gradient norm: 43364.80859375\n",
            "decoder.0.weight gradient norm: 1026723.625\n",
            "decoder.0.bias gradient norm: 31790.3203125\n",
            "decoder.2.weight gradient norm: 253369.78125\n",
            "decoder.2.bias gradient norm: 16636.361328125\n",
            "classifier.0.weight gradient norm: 2.225579023361206\n",
            "classifier.0.bias gradient norm: 0.07864116877317429\n",
            "classifier.2.weight gradient norm: 1.185335397720337\n",
            "classifier.2.bias gradient norm: 0.05136668309569359\n",
            "classifier.4.weight gradient norm: 0.8735896348953247\n",
            "classifier.4.bias gradient norm: 0.052628085017204285\n",
            "classifier.6.weight gradient norm: 0.9434298276901245\n",
            "classifier.6.bias gradient norm: 0.08026906847953796\n",
            "Epoch 33, Accuracy: 5.777777777777778%\n",
            "Epoch 34, Total Loss: 323974.15625, Classification Loss: 4.294633388519287, Reconstruction Loss: 323969.875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1543871.0\n",
            "encoder.0.bias gradient norm: 53628.5078125\n",
            "decoder.0.weight gradient norm: 1020062.375\n",
            "decoder.0.bias gradient norm: 37219.46484375\n",
            "decoder.2.weight gradient norm: 173021.125\n",
            "decoder.2.bias gradient norm: 18070.595703125\n",
            "classifier.0.weight gradient norm: 1.633217453956604\n",
            "classifier.0.bias gradient norm: 0.04909571632742882\n",
            "classifier.2.weight gradient norm: 0.874413788318634\n",
            "classifier.2.bias gradient norm: 0.033769432455301285\n",
            "classifier.4.weight gradient norm: 0.4599558413028717\n",
            "classifier.4.bias gradient norm: 0.03386532887816429\n",
            "classifier.6.weight gradient norm: 0.4107692241668701\n",
            "classifier.6.bias gradient norm: 0.04668320715427399\n",
            "Epoch 34, Accuracy: 2.4444444444444446%\n",
            "Epoch 35, Total Loss: 227676.25, Classification Loss: 4.562067985534668, Reconstruction Loss: 227671.6875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1156335.875\n",
            "encoder.0.bias gradient norm: 41513.90234375\n",
            "decoder.0.weight gradient norm: 1061667.75\n",
            "decoder.0.bias gradient norm: 31527.611328125\n",
            "decoder.2.weight gradient norm: 266953.9375\n",
            "decoder.2.bias gradient norm: 16630.30859375\n",
            "classifier.0.weight gradient norm: 3.971473455429077\n",
            "classifier.0.bias gradient norm: 0.13459204137325287\n",
            "classifier.2.weight gradient norm: 1.825423240661621\n",
            "classifier.2.bias gradient norm: 0.07646233588457108\n",
            "classifier.4.weight gradient norm: 1.3027958869934082\n",
            "classifier.4.bias gradient norm: 0.07006493210792542\n",
            "classifier.6.weight gradient norm: 1.8461956977844238\n",
            "classifier.6.bias gradient norm: 0.11336642503738403\n",
            "Epoch 35, Accuracy: 3.111111111111111%\n",
            "Epoch 36, Total Loss: 268364.5625, Classification Loss: 4.386249542236328, Reconstruction Loss: 268360.1875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1214756.375\n",
            "encoder.0.bias gradient norm: 42101.86328125\n",
            "decoder.0.weight gradient norm: 880391.75\n",
            "decoder.0.bias gradient norm: 32511.84765625\n",
            "decoder.2.weight gradient norm: 152200.75\n",
            "decoder.2.bias gradient norm: 16640.859375\n",
            "classifier.0.weight gradient norm: 2.2617204189300537\n",
            "classifier.0.bias gradient norm: 0.0731787458062172\n",
            "classifier.2.weight gradient norm: 1.1332547664642334\n",
            "classifier.2.bias gradient norm: 0.047204431146383286\n",
            "classifier.4.weight gradient norm: 0.6235945224761963\n",
            "classifier.4.bias gradient norm: 0.04531054198741913\n",
            "classifier.6.weight gradient norm: 0.667390763759613\n",
            "classifier.6.bias gradient norm: 0.06648796051740646\n",
            "Epoch 36, Accuracy: 3.3333333333333335%\n",
            "Epoch 37, Total Loss: 229128.5, Classification Loss: 4.293091297149658, Reconstruction Loss: 229124.203125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1086395.75\n",
            "encoder.0.bias gradient norm: 38952.65625\n",
            "decoder.0.weight gradient norm: 1022939.5\n",
            "decoder.0.bias gradient norm: 31218.673828125\n",
            "decoder.2.weight gradient norm: 260114.984375\n",
            "decoder.2.bias gradient norm: 16589.30078125\n",
            "classifier.0.weight gradient norm: 2.6545698642730713\n",
            "classifier.0.bias gradient norm: 0.09174151718616486\n",
            "classifier.2.weight gradient norm: 1.4254131317138672\n",
            "classifier.2.bias gradient norm: 0.05908044055104256\n",
            "classifier.4.weight gradient norm: 1.0155936479568481\n",
            "classifier.4.bias gradient norm: 0.05935453623533249\n",
            "classifier.6.weight gradient norm: 1.1725668907165527\n",
            "classifier.6.bias gradient norm: 0.09113884717226028\n",
            "Epoch 37, Accuracy: 7.777777777777778%\n",
            "Epoch 38, Total Loss: 238200.515625, Classification Loss: 4.197580814361572, Reconstruction Loss: 238196.3125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1096990.25\n",
            "encoder.0.bias gradient norm: 37630.70703125\n",
            "decoder.0.weight gradient norm: 886215.125\n",
            "decoder.0.bias gradient norm: 31500.791015625\n",
            "decoder.2.weight gradient norm: 150175.859375\n",
            "decoder.2.bias gradient norm: 15590.3017578125\n",
            "classifier.0.weight gradient norm: 1.317144513130188\n",
            "classifier.0.bias gradient norm: 0.04572255536913872\n",
            "classifier.2.weight gradient norm: 0.7567808032035828\n",
            "classifier.2.bias gradient norm: 0.03279227390885353\n",
            "classifier.4.weight gradient norm: 0.4618073105812073\n",
            "classifier.4.bias gradient norm: 0.0339534729719162\n",
            "classifier.6.weight gradient norm: 0.4389766454696655\n",
            "classifier.6.bias gradient norm: 0.04498954862356186\n",
            "Epoch 38, Accuracy: 4.444444444444445%\n",
            "Epoch 39, Total Loss: 238579.0625, Classification Loss: 4.383150577545166, Reconstruction Loss: 238574.671875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1188637.5\n",
            "encoder.0.bias gradient norm: 42659.375\n",
            "decoder.0.weight gradient norm: 1156336.25\n",
            "decoder.0.bias gradient norm: 34464.86328125\n",
            "decoder.2.weight gradient norm: 274471.4375\n",
            "decoder.2.bias gradient norm: 17183.3984375\n",
            "classifier.0.weight gradient norm: 3.804415464401245\n",
            "classifier.0.bias gradient norm: 0.11245880275964737\n",
            "classifier.2.weight gradient norm: 1.7015693187713623\n",
            "classifier.2.bias gradient norm: 0.06697066873311996\n",
            "classifier.4.weight gradient norm: 1.1966348886489868\n",
            "classifier.4.bias gradient norm: 0.06576108932495117\n",
            "classifier.6.weight gradient norm: 1.3057963848114014\n",
            "classifier.6.bias gradient norm: 0.08998328447341919\n",
            "Epoch 39, Accuracy: 2.4444444444444446%\n",
            "Epoch 40, Total Loss: 241990.546875, Classification Loss: 5.379519939422607, Reconstruction Loss: 241985.171875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1049634.75\n",
            "encoder.0.bias gradient norm: 36089.65234375\n",
            "decoder.0.weight gradient norm: 873828.5625\n",
            "decoder.0.bias gradient norm: 30509.4609375\n",
            "decoder.2.weight gradient norm: 171102.484375\n",
            "decoder.2.bias gradient norm: 15800.142578125\n",
            "classifier.0.weight gradient norm: 6.126389980316162\n",
            "classifier.0.bias gradient norm: 0.24429534375667572\n",
            "classifier.2.weight gradient norm: 3.0639777183532715\n",
            "classifier.2.bias gradient norm: 0.13787923753261566\n",
            "classifier.4.weight gradient norm: 2.7254374027252197\n",
            "classifier.4.bias gradient norm: 0.13729336857795715\n",
            "classifier.6.weight gradient norm: 5.425516128540039\n",
            "classifier.6.bias gradient norm: 0.24606753885746002\n",
            "Epoch 40, Accuracy: 2.0%\n",
            "Epoch 41, Total Loss: 204211.578125, Classification Loss: 4.499177932739258, Reconstruction Loss: 204207.078125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 954891.375\n",
            "encoder.0.bias gradient norm: 34123.00390625\n",
            "decoder.0.weight gradient norm: 927179.25\n",
            "decoder.0.bias gradient norm: 27595.7578125\n",
            "decoder.2.weight gradient norm: 225833.6875\n",
            "decoder.2.bias gradient norm: 15235.6416015625\n",
            "classifier.0.weight gradient norm: 3.937408208847046\n",
            "classifier.0.bias gradient norm: 0.12164095044136047\n",
            "classifier.2.weight gradient norm: 1.8170708417892456\n",
            "classifier.2.bias gradient norm: 0.07202804833650589\n",
            "classifier.4.weight gradient norm: 1.231245756149292\n",
            "classifier.4.bias gradient norm: 0.0699395164847374\n",
            "classifier.6.weight gradient norm: 1.3618525266647339\n",
            "classifier.6.bias gradient norm: 0.10166769474744797\n",
            "Epoch 41, Accuracy: 1.5555555555555556%\n",
            "Epoch 42, Total Loss: 236368.28125, Classification Loss: 4.650201797485352, Reconstruction Loss: 236363.625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1137642.125\n",
            "encoder.0.bias gradient norm: 39175.9296875\n",
            "decoder.0.weight gradient norm: 910379.625\n",
            "decoder.0.bias gradient norm: 31487.1171875\n",
            "decoder.2.weight gradient norm: 150152.234375\n",
            "decoder.2.bias gradient norm: 15620.5703125\n",
            "classifier.0.weight gradient norm: 3.181291341781616\n",
            "classifier.0.bias gradient norm: 0.12460899353027344\n",
            "classifier.2.weight gradient norm: 1.5716322660446167\n",
            "classifier.2.bias gradient norm: 0.07277188450098038\n",
            "classifier.4.weight gradient norm: 1.219104290008545\n",
            "classifier.4.bias gradient norm: 0.07400842756032944\n",
            "classifier.6.weight gradient norm: 1.7844828367233276\n",
            "classifier.6.bias gradient norm: 0.1188134029507637\n",
            "Epoch 42, Accuracy: 2.4444444444444446%\n",
            "Epoch 43, Total Loss: 240052.96875, Classification Loss: 4.947516918182373, Reconstruction Loss: 240048.015625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1177999.125\n",
            "encoder.0.bias gradient norm: 42582.30859375\n",
            "decoder.0.weight gradient norm: 1166956.875\n",
            "decoder.0.bias gradient norm: 34427.59765625\n",
            "decoder.2.weight gradient norm: 264891.53125\n",
            "decoder.2.bias gradient norm: 17362.55078125\n",
            "classifier.0.weight gradient norm: 6.139763355255127\n",
            "classifier.0.bias gradient norm: 0.16660434007644653\n",
            "classifier.2.weight gradient norm: 2.714355707168579\n",
            "classifier.2.bias gradient norm: 0.09491432458162308\n",
            "classifier.4.weight gradient norm: 1.985770344734192\n",
            "classifier.4.bias gradient norm: 0.0940643697977066\n",
            "classifier.6.weight gradient norm: 2.6139471530914307\n",
            "classifier.6.bias gradient norm: 0.1425725668668747\n",
            "Epoch 43, Accuracy: 2.6666666666666665%\n",
            "Epoch 44, Total Loss: 242245.1875, Classification Loss: 5.139164447784424, Reconstruction Loss: 242240.046875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 935536.3125\n",
            "encoder.0.bias gradient norm: 32345.990234375\n",
            "decoder.0.weight gradient norm: 851330.5625\n",
            "decoder.0.bias gradient norm: 29191.68359375\n",
            "decoder.2.weight gradient norm: 164996.34375\n",
            "decoder.2.bias gradient norm: 15718.125\n",
            "classifier.0.weight gradient norm: 6.0203938484191895\n",
            "classifier.0.bias gradient norm: 0.23438401520252228\n",
            "classifier.2.weight gradient norm: 2.8249809741973877\n",
            "classifier.2.bias gradient norm: 0.1267932653427124\n",
            "classifier.4.weight gradient norm: 2.4602224826812744\n",
            "classifier.4.bias gradient norm: 0.11900810152292252\n",
            "classifier.6.weight gradient norm: 4.484130382537842\n",
            "classifier.6.bias gradient norm: 0.2043871134519577\n",
            "Epoch 44, Accuracy: 3.111111111111111%\n",
            "Epoch 45, Total Loss: 198390.25, Classification Loss: 4.2728166580200195, Reconstruction Loss: 198385.984375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 948230.3125\n",
            "encoder.0.bias gradient norm: 34276.4140625\n",
            "decoder.0.weight gradient norm: 929440.9375\n",
            "decoder.0.bias gradient norm: 28194.984375\n",
            "decoder.2.weight gradient norm: 226139.765625\n",
            "decoder.2.bias gradient norm: 15130.3583984375\n",
            "classifier.0.weight gradient norm: 2.4458470344543457\n",
            "classifier.0.bias gradient norm: 0.06134830787777901\n",
            "classifier.2.weight gradient norm: 1.1526589393615723\n",
            "classifier.2.bias gradient norm: 0.03880230337381363\n",
            "classifier.4.weight gradient norm: 0.7488094568252563\n",
            "classifier.4.bias gradient norm: 0.04137122631072998\n",
            "classifier.6.weight gradient norm: 0.738591194152832\n",
            "classifier.6.bias gradient norm: 0.06388626992702484\n",
            "Epoch 45, Accuracy: 3.5555555555555554%\n",
            "Epoch 46, Total Loss: 233244.0, Classification Loss: 4.584341526031494, Reconstruction Loss: 233239.421875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1029032.1875\n",
            "encoder.0.bias gradient norm: 35647.8671875\n",
            "decoder.0.weight gradient norm: 851583.9375\n",
            "decoder.0.bias gradient norm: 29569.517578125\n",
            "decoder.2.weight gradient norm: 146313.8125\n",
            "decoder.2.bias gradient norm: 15615.9111328125\n",
            "classifier.0.weight gradient norm: 3.4931106567382812\n",
            "classifier.0.bias gradient norm: 0.14133499562740326\n",
            "classifier.2.weight gradient norm: 1.6538258790969849\n",
            "classifier.2.bias gradient norm: 0.07824400812387466\n",
            "classifier.4.weight gradient norm: 1.2819386720657349\n",
            "classifier.4.bias gradient norm: 0.07440716028213501\n",
            "classifier.6.weight gradient norm: 1.9674758911132812\n",
            "classifier.6.bias gradient norm: 0.12048189342021942\n",
            "Epoch 46, Accuracy: 5.555555555555555%\n",
            "Epoch 47, Total Loss: 249479.890625, Classification Loss: 4.177299976348877, Reconstruction Loss: 249475.71875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1204906.0\n",
            "encoder.0.bias gradient norm: 43229.296875\n",
            "decoder.0.weight gradient norm: 1197693.25\n",
            "decoder.0.bias gradient norm: 35109.1171875\n",
            "decoder.2.weight gradient norm: 271651.3125\n",
            "decoder.2.bias gradient norm: 17738.900390625\n",
            "classifier.0.weight gradient norm: 2.5552151203155518\n",
            "classifier.0.bias gradient norm: 0.06907455623149872\n",
            "classifier.2.weight gradient norm: 1.1416593790054321\n",
            "classifier.2.bias gradient norm: 0.04088744521141052\n",
            "classifier.4.weight gradient norm: 0.7693469524383545\n",
            "classifier.4.bias gradient norm: 0.04140307754278183\n",
            "classifier.6.weight gradient norm: 0.8117879629135132\n",
            "classifier.6.bias gradient norm: 0.06428482383489609\n",
            "Epoch 47, Accuracy: 2.0%\n",
            "Epoch 48, Total Loss: 237574.359375, Classification Loss: 5.069228649139404, Reconstruction Loss: 237569.296875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 950801.125\n",
            "encoder.0.bias gradient norm: 32790.2109375\n",
            "decoder.0.weight gradient norm: 867703.8125\n",
            "decoder.0.bias gradient norm: 28251.119140625\n",
            "decoder.2.weight gradient norm: 170465.875\n",
            "decoder.2.bias gradient norm: 15626.236328125\n",
            "classifier.0.weight gradient norm: 3.951497793197632\n",
            "classifier.0.bias gradient norm: 0.13882401585578918\n",
            "classifier.2.weight gradient norm: 1.9681284427642822\n",
            "classifier.2.bias gradient norm: 0.0841078832745552\n",
            "classifier.4.weight gradient norm: 2.0509841442108154\n",
            "classifier.4.bias gradient norm: 0.0952131450176239\n",
            "classifier.6.weight gradient norm: 3.1061105728149414\n",
            "classifier.6.bias gradient norm: 0.153471902012825\n",
            "Epoch 48, Accuracy: 3.7777777777777777%\n",
            "Epoch 49, Total Loss: 229472.328125, Classification Loss: 4.258596420288086, Reconstruction Loss: 229468.0625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1115683.0\n",
            "encoder.0.bias gradient norm: 40010.6796875\n",
            "decoder.0.weight gradient norm: 1075606.0\n",
            "decoder.0.bias gradient norm: 31893.103515625\n",
            "decoder.2.weight gradient norm: 254845.875\n",
            "decoder.2.bias gradient norm: 16790.197265625\n",
            "classifier.0.weight gradient norm: 2.899925470352173\n",
            "classifier.0.bias gradient norm: 0.09904829412698746\n",
            "classifier.2.weight gradient norm: 1.2873508930206299\n",
            "classifier.2.bias gradient norm: 0.055671896785497665\n",
            "classifier.4.weight gradient norm: 0.9264490008354187\n",
            "classifier.4.bias gradient norm: 0.055059950798749924\n",
            "classifier.6.weight gradient norm: 1.0817078351974487\n",
            "classifier.6.bias gradient norm: 0.08042937517166138\n",
            "Epoch 49, Accuracy: 4.888888888888889%\n",
            "Epoch 50, Total Loss: 221078.53125, Classification Loss: 4.183918476104736, Reconstruction Loss: 221074.34375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 963610.875\n",
            "encoder.0.bias gradient norm: 33138.5546875\n",
            "decoder.0.weight gradient norm: 786856.5625\n",
            "decoder.0.bias gradient norm: 26893.09375\n",
            "decoder.2.weight gradient norm: 160347.671875\n",
            "decoder.2.bias gradient norm: 14995.83984375\n",
            "classifier.0.weight gradient norm: 2.140159845352173\n",
            "classifier.0.bias gradient norm: 0.06845543533563614\n",
            "classifier.2.weight gradient norm: 0.9926515221595764\n",
            "classifier.2.bias gradient norm: 0.041730429977178574\n",
            "classifier.4.weight gradient norm: 0.6548389792442322\n",
            "classifier.4.bias gradient norm: 0.04292871430516243\n",
            "classifier.6.weight gradient norm: 0.7397037744522095\n",
            "classifier.6.bias gradient norm: 0.06133886054158211\n",
            "Epoch 50, Accuracy: 4.666666666666667%\n",
            "Epoch 51, Total Loss: 222102.59375, Classification Loss: 4.168375015258789, Reconstruction Loss: 222098.421875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1075244.375\n",
            "encoder.0.bias gradient norm: 38775.1640625\n",
            "decoder.0.weight gradient norm: 1061747.25\n",
            "decoder.0.bias gradient norm: 31575.94140625\n",
            "decoder.2.weight gradient norm: 241950.0\n",
            "decoder.2.bias gradient norm: 16433.021484375\n",
            "classifier.0.weight gradient norm: 2.7834527492523193\n",
            "classifier.0.bias gradient norm: 0.09016355127096176\n",
            "classifier.2.weight gradient norm: 1.3182376623153687\n",
            "classifier.2.bias gradient norm: 0.05367260426282883\n",
            "classifier.4.weight gradient norm: 1.0613384246826172\n",
            "classifier.4.bias gradient norm: 0.05407751724123955\n",
            "classifier.6.weight gradient norm: 1.690450668334961\n",
            "classifier.6.bias gradient norm: 0.09689413011074066\n",
            "Epoch 51, Accuracy: 5.333333333333333%\n",
            "Epoch 52, Total Loss: 229263.1875, Classification Loss: 4.52943229675293, Reconstruction Loss: 229258.65625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 988249.875\n",
            "encoder.0.bias gradient norm: 33975.69921875\n",
            "decoder.0.weight gradient norm: 875632.8125\n",
            "decoder.0.bias gradient norm: 30436.37109375\n",
            "decoder.2.weight gradient norm: 152493.359375\n",
            "decoder.2.bias gradient norm: 15401.7060546875\n",
            "classifier.0.weight gradient norm: 4.13252592086792\n",
            "classifier.0.bias gradient norm: 0.14925159513950348\n",
            "classifier.2.weight gradient norm: 1.9428820610046387\n",
            "classifier.2.bias gradient norm: 0.08955831080675125\n",
            "classifier.4.weight gradient norm: 1.2847051620483398\n",
            "classifier.4.bias gradient norm: 0.0845508798956871\n",
            "classifier.6.weight gradient norm: 1.7435966730117798\n",
            "classifier.6.bias gradient norm: 0.12359602004289627\n",
            "Epoch 52, Accuracy: 1.3333333333333333%\n",
            "Epoch 53, Total Loss: 247044.59375, Classification Loss: 5.27793025970459, Reconstruction Loss: 247039.3125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1205993.5\n",
            "encoder.0.bias gradient norm: 43764.75390625\n",
            "decoder.0.weight gradient norm: 1221181.5\n",
            "decoder.0.bias gradient norm: 37111.078125\n",
            "decoder.2.weight gradient norm: 278043.28125\n",
            "decoder.2.bias gradient norm: 17862.083984375\n",
            "classifier.0.weight gradient norm: 5.957400798797607\n",
            "classifier.0.bias gradient norm: 0.18358537554740906\n",
            "classifier.2.weight gradient norm: 3.3311092853546143\n",
            "classifier.2.bias gradient norm: 0.12453322112560272\n",
            "classifier.4.weight gradient norm: 3.373300790786743\n",
            "classifier.4.bias gradient norm: 0.15478681027889252\n",
            "classifier.6.weight gradient norm: 5.191897869110107\n",
            "classifier.6.bias gradient norm: 0.2588914930820465\n",
            "Epoch 53, Accuracy: 3.3333333333333335%\n",
            "Epoch 54, Total Loss: 224967.015625, Classification Loss: 4.617761611938477, Reconstruction Loss: 224962.390625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1092350.875\n",
            "encoder.0.bias gradient norm: 37626.47265625\n",
            "decoder.0.weight gradient norm: 870841.4375\n",
            "decoder.0.bias gradient norm: 32063.384765625\n",
            "decoder.2.weight gradient norm: 142841.75\n",
            "decoder.2.bias gradient norm: 15372.7080078125\n",
            "classifier.0.weight gradient norm: 5.016376972198486\n",
            "classifier.0.bias gradient norm: 0.18737640976905823\n",
            "classifier.2.weight gradient norm: 1.8737037181854248\n",
            "classifier.2.bias gradient norm: 0.08965273201465607\n",
            "classifier.4.weight gradient norm: 1.255454421043396\n",
            "classifier.4.bias gradient norm: 0.08233435451984406\n",
            "classifier.6.weight gradient norm: 1.7585052251815796\n",
            "classifier.6.bias gradient norm: 0.10921038687229156\n",
            "Epoch 54, Accuracy: 2.6666666666666665%\n",
            "Epoch 55, Total Loss: 243508.96875, Classification Loss: 4.953547954559326, Reconstruction Loss: 243504.015625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1124069.25\n",
            "encoder.0.bias gradient norm: 41011.11328125\n",
            "decoder.0.weight gradient norm: 1147091.625\n",
            "decoder.0.bias gradient norm: 35240.1875\n",
            "decoder.2.weight gradient norm: 282608.59375\n",
            "decoder.2.bias gradient norm: 17772.900390625\n",
            "classifier.0.weight gradient norm: 6.670405387878418\n",
            "classifier.0.bias gradient norm: 0.21720024943351746\n",
            "classifier.2.weight gradient norm: 2.747959852218628\n",
            "classifier.2.bias gradient norm: 0.10923808068037033\n",
            "classifier.4.weight gradient norm: 2.4897971153259277\n",
            "classifier.4.bias gradient norm: 0.10953326523303986\n",
            "classifier.6.weight gradient norm: 3.958772897720337\n",
            "classifier.6.bias gradient norm: 0.17779605090618134\n",
            "Epoch 55, Accuracy: 4.222222222222222%\n",
            "Epoch 56, Total Loss: 222434.828125, Classification Loss: 4.217082500457764, Reconstruction Loss: 222430.609375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1038095.75\n",
            "encoder.0.bias gradient norm: 35660.60546875\n",
            "decoder.0.weight gradient norm: 871467.875\n",
            "decoder.0.bias gradient norm: 31075.255859375\n",
            "decoder.2.weight gradient norm: 137482.96875\n",
            "decoder.2.bias gradient norm: 15240.9072265625\n",
            "classifier.0.weight gradient norm: 2.382197618484497\n",
            "classifier.0.bias gradient norm: 0.09151076525449753\n",
            "classifier.2.weight gradient norm: 1.0355116128921509\n",
            "classifier.2.bias gradient norm: 0.04903307184576988\n",
            "classifier.4.weight gradient norm: 0.6694058179855347\n",
            "classifier.4.bias gradient norm: 0.0493977852165699\n",
            "classifier.6.weight gradient norm: 0.753166913986206\n",
            "classifier.6.bias gradient norm: 0.06682948023080826\n",
            "Epoch 56, Accuracy: 2.6666666666666665%\n",
            "Epoch 57, Total Loss: 205572.765625, Classification Loss: 4.763391494750977, Reconstruction Loss: 205568.0\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1002622.6875\n",
            "encoder.0.bias gradient norm: 36129.3203125\n",
            "decoder.0.weight gradient norm: 977762.9375\n",
            "decoder.0.bias gradient norm: 29937.083984375\n",
            "decoder.2.weight gradient norm: 245917.890625\n",
            "decoder.2.bias gradient norm: 15674.3935546875\n",
            "classifier.0.weight gradient norm: 5.3242597579956055\n",
            "classifier.0.bias gradient norm: 0.1685389280319214\n",
            "classifier.2.weight gradient norm: 2.2738802433013916\n",
            "classifier.2.bias gradient norm: 0.08980938792228699\n",
            "classifier.4.weight gradient norm: 1.9946675300598145\n",
            "classifier.4.bias gradient norm: 0.09482257813215256\n",
            "classifier.6.weight gradient norm: 2.7531514167785645\n",
            "classifier.6.bias gradient norm: 0.14166323840618134\n",
            "Epoch 57, Accuracy: 3.7777777777777777%\n",
            "Epoch 58, Total Loss: 213167.390625, Classification Loss: 4.371111869812012, Reconstruction Loss: 213163.015625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 851936.5625\n",
            "encoder.0.bias gradient norm: 29181.84765625\n",
            "decoder.0.weight gradient norm: 776896.875\n",
            "decoder.0.bias gradient norm: 26721.5\n",
            "decoder.2.weight gradient norm: 154095.65625\n",
            "decoder.2.bias gradient norm: 14838.6142578125\n",
            "classifier.0.weight gradient norm: 3.5493664741516113\n",
            "classifier.0.bias gradient norm: 0.12955795228481293\n",
            "classifier.2.weight gradient norm: 1.6545544862747192\n",
            "classifier.2.bias gradient norm: 0.07358753681182861\n",
            "classifier.4.weight gradient norm: 1.2482515573501587\n",
            "classifier.4.bias gradient norm: 0.07135193049907684\n",
            "classifier.6.weight gradient norm: 1.5509403944015503\n",
            "classifier.6.bias gradient norm: 0.09914442896842957\n",
            "Epoch 58, Accuracy: 3.5555555555555554%\n",
            "Epoch 59, Total Loss: 190805.40625, Classification Loss: 4.592000961303711, Reconstruction Loss: 190800.8125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 857533.6875\n",
            "encoder.0.bias gradient norm: 30935.82421875\n",
            "decoder.0.weight gradient norm: 864039.0\n",
            "decoder.0.bias gradient norm: 27159.84375\n",
            "decoder.2.weight gradient norm: 204270.984375\n",
            "decoder.2.bias gradient norm: 14705.62109375\n",
            "classifier.0.weight gradient norm: 3.8052635192871094\n",
            "classifier.0.bias gradient norm: 0.13144977390766144\n",
            "classifier.2.weight gradient norm: 1.8389911651611328\n",
            "classifier.2.bias gradient norm: 0.07338083535432816\n",
            "classifier.4.weight gradient norm: 1.7998532056808472\n",
            "classifier.4.bias gradient norm: 0.07691886276006699\n",
            "classifier.6.weight gradient norm: 2.689634323120117\n",
            "classifier.6.bias gradient norm: 0.12839777767658234\n",
            "Epoch 59, Accuracy: 1.7777777777777777%\n",
            "Epoch 60, Total Loss: 220173.859375, Classification Loss: 4.8614821434021, Reconstruction Loss: 220169.0\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1008102.625\n",
            "encoder.0.bias gradient norm: 34354.8984375\n",
            "decoder.0.weight gradient norm: 858210.125\n",
            "decoder.0.bias gradient norm: 28520.353515625\n",
            "decoder.2.weight gradient norm: 163136.1875\n",
            "decoder.2.bias gradient norm: 15035.451171875\n",
            "classifier.0.weight gradient norm: 5.41334342956543\n",
            "classifier.0.bias gradient norm: 0.1842002123594284\n",
            "classifier.2.weight gradient norm: 2.524230480194092\n",
            "classifier.2.bias gradient norm: 0.10693786293268204\n",
            "classifier.4.weight gradient norm: 2.1978390216827393\n",
            "classifier.4.bias gradient norm: 0.115123450756073\n",
            "classifier.6.weight gradient norm: 3.6855318546295166\n",
            "classifier.6.bias gradient norm: 0.1942972093820572\n",
            "Epoch 60, Accuracy: 3.5555555555555554%\n",
            "Epoch 61, Total Loss: 221968.890625, Classification Loss: 4.608514308929443, Reconstruction Loss: 221964.28125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1076517.875\n",
            "encoder.0.bias gradient norm: 38882.328125\n",
            "decoder.0.weight gradient norm: 1042681.25\n",
            "decoder.0.bias gradient norm: 32201.943359375\n",
            "decoder.2.weight gradient norm: 248060.859375\n",
            "decoder.2.bias gradient norm: 16538.60546875\n",
            "classifier.0.weight gradient norm: 6.8894429206848145\n",
            "classifier.0.bias gradient norm: 0.21846690773963928\n",
            "classifier.2.weight gradient norm: 2.9205071926116943\n",
            "classifier.2.bias gradient norm: 0.11223714053630829\n",
            "classifier.4.weight gradient norm: 2.4019880294799805\n",
            "classifier.4.bias gradient norm: 0.11227088421583176\n",
            "classifier.6.weight gradient norm: 3.1352767944335938\n",
            "classifier.6.bias gradient norm: 0.17432838678359985\n",
            "Epoch 61, Accuracy: 2.2222222222222223%\n",
            "Epoch 62, Total Loss: 242362.21875, Classification Loss: 4.634345531463623, Reconstruction Loss: 242357.578125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1298176.375\n",
            "encoder.0.bias gradient norm: 44513.46875\n",
            "decoder.0.weight gradient norm: 987786.6875\n",
            "decoder.0.bias gradient norm: 34119.04296875\n",
            "decoder.2.weight gradient norm: 156510.15625\n",
            "decoder.2.bias gradient norm: 15902.1083984375\n",
            "classifier.0.weight gradient norm: 3.946350574493408\n",
            "classifier.0.bias gradient norm: 0.1490488201379776\n",
            "classifier.2.weight gradient norm: 2.0069446563720703\n",
            "classifier.2.bias gradient norm: 0.08928252011537552\n",
            "classifier.4.weight gradient norm: 2.157431125640869\n",
            "classifier.4.bias gradient norm: 0.10094057023525238\n",
            "classifier.6.weight gradient norm: 3.238360643386841\n",
            "classifier.6.bias gradient norm: 0.16617588698863983\n",
            "Epoch 62, Accuracy: 2.888888888888889%\n",
            "Epoch 63, Total Loss: 213331.609375, Classification Loss: 4.509932994842529, Reconstruction Loss: 213327.09375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1036748.8125\n",
            "encoder.0.bias gradient norm: 37759.0859375\n",
            "decoder.0.weight gradient norm: 1009398.875\n",
            "decoder.0.bias gradient norm: 32051.80078125\n",
            "decoder.2.weight gradient norm: 237250.25\n",
            "decoder.2.bias gradient norm: 16199.7421875\n",
            "classifier.0.weight gradient norm: 4.227620601654053\n",
            "classifier.0.bias gradient norm: 0.12989723682403564\n",
            "classifier.2.weight gradient norm: 2.1358842849731445\n",
            "classifier.2.bias gradient norm: 0.08437629789113998\n",
            "classifier.4.weight gradient norm: 1.8567513227462769\n",
            "classifier.4.bias gradient norm: 0.09578697383403778\n",
            "classifier.6.weight gradient norm: 2.2134785652160645\n",
            "classifier.6.bias gradient norm: 0.14243049919605255\n",
            "Epoch 63, Accuracy: 3.7777777777777777%\n",
            "Epoch 64, Total Loss: 207305.359375, Classification Loss: 4.113466739654541, Reconstruction Loss: 207301.25\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 948392.25\n",
            "encoder.0.bias gradient norm: 32590.943359375\n",
            "decoder.0.weight gradient norm: 788108.4375\n",
            "decoder.0.bias gradient norm: 28903.919921875\n",
            "decoder.2.weight gradient norm: 144424.234375\n",
            "decoder.2.bias gradient norm: 14572.83984375\n",
            "classifier.0.weight gradient norm: 2.696012258529663\n",
            "classifier.0.bias gradient norm: 0.10096147656440735\n",
            "classifier.2.weight gradient norm: 1.2401479482650757\n",
            "classifier.2.bias gradient norm: 0.05491619557142258\n",
            "classifier.4.weight gradient norm: 0.7942621111869812\n",
            "classifier.4.bias gradient norm: 0.0552903488278389\n",
            "classifier.6.weight gradient norm: 0.8260989785194397\n",
            "classifier.6.bias gradient norm: 0.07501498609781265\n",
            "Epoch 64, Accuracy: 2.4444444444444446%\n",
            "Epoch 65, Total Loss: 222756.609375, Classification Loss: 6.152761459350586, Reconstruction Loss: 222750.453125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1095729.125\n",
            "encoder.0.bias gradient norm: 39861.76171875\n",
            "decoder.0.weight gradient norm: 1079262.5\n",
            "decoder.0.bias gradient norm: 33904.46875\n",
            "decoder.2.weight gradient norm: 242636.546875\n",
            "decoder.2.bias gradient norm: 16777.203125\n",
            "classifier.0.weight gradient norm: 7.845361232757568\n",
            "classifier.0.bias gradient norm: 0.26666849851608276\n",
            "classifier.2.weight gradient norm: 3.690861701965332\n",
            "classifier.2.bias gradient norm: 0.1435903161764145\n",
            "classifier.4.weight gradient norm: 4.300063610076904\n",
            "classifier.4.bias gradient norm: 0.14252685010433197\n",
            "classifier.6.weight gradient norm: 7.328069686889648\n",
            "classifier.6.bias gradient norm: 0.22482255101203918\n",
            "Epoch 65, Accuracy: 4.666666666666667%\n",
            "Epoch 66, Total Loss: 210132.8125, Classification Loss: 4.176519870758057, Reconstruction Loss: 210128.640625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 943164.0625\n",
            "encoder.0.bias gradient norm: 32379.828125\n",
            "decoder.0.weight gradient norm: 883316.0625\n",
            "decoder.0.bias gradient norm: 29955.84375\n",
            "decoder.2.weight gradient norm: 155951.703125\n",
            "decoder.2.bias gradient norm: 14647.923828125\n",
            "classifier.0.weight gradient norm: 2.9321649074554443\n",
            "classifier.0.bias gradient norm: 0.09158096462488174\n",
            "classifier.2.weight gradient norm: 1.324104905128479\n",
            "classifier.2.bias gradient norm: 0.05045907571911812\n",
            "classifier.4.weight gradient norm: 0.9890008568763733\n",
            "classifier.4.bias gradient norm: 0.05563538148999214\n",
            "classifier.6.weight gradient norm: 1.2962925434112549\n",
            "classifier.6.bias gradient norm: 0.08749838918447495\n",
            "Epoch 66, Accuracy: 4.666666666666667%\n",
            "Epoch 67, Total Loss: 223732.5625, Classification Loss: 4.7166266441345215, Reconstruction Loss: 223727.84375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1060356.125\n",
            "encoder.0.bias gradient norm: 38345.75390625\n",
            "decoder.0.weight gradient norm: 1004757.9375\n",
            "decoder.0.bias gradient norm: 31647.28125\n",
            "decoder.2.weight gradient norm: 236920.46875\n",
            "decoder.2.bias gradient norm: 16721.349609375\n",
            "classifier.0.weight gradient norm: 3.6218278408050537\n",
            "classifier.0.bias gradient norm: 0.12537293136119843\n",
            "classifier.2.weight gradient norm: 1.7686935663223267\n",
            "classifier.2.bias gradient norm: 0.07071072608232498\n",
            "classifier.4.weight gradient norm: 1.724346399307251\n",
            "classifier.4.bias gradient norm: 0.0751367136836052\n",
            "classifier.6.weight gradient norm: 2.6137475967407227\n",
            "classifier.6.bias gradient norm: 0.11995658278465271\n",
            "Epoch 67, Accuracy: 2.6666666666666665%\n",
            "Epoch 68, Total Loss: 205947.046875, Classification Loss: 4.868941307067871, Reconstruction Loss: 205942.171875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1015865.8125\n",
            "encoder.0.bias gradient norm: 35012.140625\n",
            "decoder.0.weight gradient norm: 805523.9375\n",
            "decoder.0.bias gradient norm: 28404.232421875\n",
            "decoder.2.weight gradient norm: 152060.96875\n",
            "decoder.2.bias gradient norm: 14563.5146484375\n",
            "classifier.0.weight gradient norm: 6.1911821365356445\n",
            "classifier.0.bias gradient norm: 0.2341172695159912\n",
            "classifier.2.weight gradient norm: 2.8917713165283203\n",
            "classifier.2.bias gradient norm: 0.12278389185667038\n",
            "classifier.4.weight gradient norm: 3.203920841217041\n",
            "classifier.4.bias gradient norm: 0.1460510641336441\n",
            "classifier.6.weight gradient norm: 5.798322677612305\n",
            "classifier.6.bias gradient norm: 0.27768200635910034\n",
            "Epoch 68, Accuracy: 4.666666666666667%\n",
            "Epoch 69, Total Loss: 203126.1875, Classification Loss: 4.658827781677246, Reconstruction Loss: 203121.53125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 974672.0\n",
            "encoder.0.bias gradient norm: 35293.4453125\n",
            "decoder.0.weight gradient norm: 950501.3125\n",
            "decoder.0.bias gradient norm: 29937.60546875\n",
            "decoder.2.weight gradient norm: 218380.671875\n",
            "decoder.2.bias gradient norm: 15627.2265625\n",
            "classifier.0.weight gradient norm: 4.2676520347595215\n",
            "classifier.0.bias gradient norm: 0.14674191176891327\n",
            "classifier.2.weight gradient norm: 1.8839036226272583\n",
            "classifier.2.bias gradient norm: 0.07561999559402466\n",
            "classifier.4.weight gradient norm: 1.5474705696105957\n",
            "classifier.4.bias gradient norm: 0.07666720449924469\n",
            "classifier.6.weight gradient norm: 2.122365713119507\n",
            "classifier.6.bias gradient norm: 0.11524586379528046\n",
            "Epoch 69, Accuracy: 2.888888888888889%\n",
            "Epoch 70, Total Loss: 183031.046875, Classification Loss: 4.933474540710449, Reconstruction Loss: 183026.109375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 927006.875\n",
            "encoder.0.bias gradient norm: 31819.51171875\n",
            "decoder.0.weight gradient norm: 709667.625\n",
            "decoder.0.bias gradient norm: 26007.25390625\n",
            "decoder.2.weight gradient norm: 130729.984375\n",
            "decoder.2.bias gradient norm: 13714.5712890625\n",
            "classifier.0.weight gradient norm: 5.135481834411621\n",
            "classifier.0.bias gradient norm: 0.19465471804141998\n",
            "classifier.2.weight gradient norm: 2.4201273918151855\n",
            "classifier.2.bias gradient norm: 0.10865696519613266\n",
            "classifier.4.weight gradient norm: 2.602027177810669\n",
            "classifier.4.bias gradient norm: 0.12449466437101364\n",
            "classifier.6.weight gradient norm: 4.129884243011475\n",
            "classifier.6.bias gradient norm: 0.1963886022567749\n",
            "Epoch 70, Accuracy: 4.222222222222222%\n",
            "Epoch 71, Total Loss: 250576.65625, Classification Loss: 4.408020973205566, Reconstruction Loss: 250572.25\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1221322.875\n",
            "encoder.0.bias gradient norm: 44740.98828125\n",
            "decoder.0.weight gradient norm: 1301262.625\n",
            "decoder.0.bias gradient norm: 40363.82421875\n",
            "decoder.2.weight gradient norm: 279342.15625\n",
            "decoder.2.bias gradient norm: 18263.294921875\n",
            "classifier.0.weight gradient norm: 3.2709336280822754\n",
            "classifier.0.bias gradient norm: 0.11174307018518448\n",
            "classifier.2.weight gradient norm: 1.510785460472107\n",
            "classifier.2.bias gradient norm: 0.05945857614278793\n",
            "classifier.4.weight gradient norm: 1.4636330604553223\n",
            "classifier.4.bias gradient norm: 0.06751465797424316\n",
            "classifier.6.weight gradient norm: 2.0395963191986084\n",
            "classifier.6.bias gradient norm: 0.10553539544343948\n",
            "Epoch 71, Accuracy: 2.0%\n",
            "Epoch 72, Total Loss: 213178.5, Classification Loss: 4.560717582702637, Reconstruction Loss: 213173.9375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1056033.25\n",
            "encoder.0.bias gradient norm: 36514.5625\n",
            "decoder.0.weight gradient norm: 842871.25\n",
            "decoder.0.bias gradient norm: 31555.001953125\n",
            "decoder.2.weight gradient norm: 143146.046875\n",
            "decoder.2.bias gradient norm: 15059.919921875\n",
            "classifier.0.weight gradient norm: 2.9015445709228516\n",
            "classifier.0.bias gradient norm: 0.11555734276771545\n",
            "classifier.2.weight gradient norm: 1.651373028755188\n",
            "classifier.2.bias gradient norm: 0.07326304912567139\n",
            "classifier.4.weight gradient norm: 2.008692979812622\n",
            "classifier.4.bias gradient norm: 0.08601181954145432\n",
            "classifier.6.weight gradient norm: 2.878603935241699\n",
            "classifier.6.bias gradient norm: 0.14378441870212555\n",
            "Epoch 72, Accuracy: 8.222222222222221%\n",
            "Epoch 73, Total Loss: 238715.421875, Classification Loss: 4.185957431793213, Reconstruction Loss: 238711.234375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1146259.0\n",
            "encoder.0.bias gradient norm: 42110.18359375\n",
            "decoder.0.weight gradient norm: 1215562.25\n",
            "decoder.0.bias gradient norm: 38556.09765625\n",
            "decoder.2.weight gradient norm: 266247.75\n",
            "decoder.2.bias gradient norm: 17711.962890625\n",
            "classifier.0.weight gradient norm: 2.265977382659912\n",
            "classifier.0.bias gradient norm: 0.07368552684783936\n",
            "classifier.2.weight gradient norm: 1.1210461854934692\n",
            "classifier.2.bias gradient norm: 0.044499482959508896\n",
            "classifier.4.weight gradient norm: 0.9604398012161255\n",
            "classifier.4.bias gradient norm: 0.05070282146334648\n",
            "classifier.6.weight gradient norm: 0.9663260579109192\n",
            "classifier.6.bias gradient norm: 0.07458357512950897\n",
            "Epoch 73, Accuracy: 4.888888888888889%\n",
            "Epoch 74, Total Loss: 181981.53125, Classification Loss: 4.278039932250977, Reconstruction Loss: 181977.25\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 921206.5\n",
            "encoder.0.bias gradient norm: 31658.4921875\n",
            "decoder.0.weight gradient norm: 742866.5\n",
            "decoder.0.bias gradient norm: 28454.666015625\n",
            "decoder.2.weight gradient norm: 117095.640625\n",
            "decoder.2.bias gradient norm: 13706.419921875\n",
            "classifier.0.weight gradient norm: 3.467416524887085\n",
            "classifier.0.bias gradient norm: 0.1351635903120041\n",
            "classifier.2.weight gradient norm: 1.4801820516586304\n",
            "classifier.2.bias gradient norm: 0.06840740144252777\n",
            "classifier.4.weight gradient norm: 1.1098953485488892\n",
            "classifier.4.bias gradient norm: 0.06211801618337631\n",
            "classifier.6.weight gradient norm: 1.1863837242126465\n",
            "classifier.6.bias gradient norm: 0.08207453042268753\n",
            "Epoch 74, Accuracy: 2.6666666666666665%\n",
            "Epoch 75, Total Loss: 206851.265625, Classification Loss: 5.806454658508301, Reconstruction Loss: 206845.453125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 907343.75\n",
            "encoder.0.bias gradient norm: 33762.36328125\n",
            "decoder.0.weight gradient norm: 992083.5\n",
            "decoder.0.bias gradient norm: 31160.087890625\n",
            "decoder.2.weight gradient norm: 241017.09375\n",
            "decoder.2.bias gradient norm: 16033.2607421875\n",
            "classifier.0.weight gradient norm: 9.253540992736816\n",
            "classifier.0.bias gradient norm: 0.3053106367588043\n",
            "classifier.2.weight gradient norm: 4.6647868156433105\n",
            "classifier.2.bias gradient norm: 0.17220424115657806\n",
            "classifier.4.weight gradient norm: 5.219030857086182\n",
            "classifier.4.bias gradient norm: 0.19280673563480377\n",
            "classifier.6.weight gradient norm: 10.480527877807617\n",
            "classifier.6.bias gradient norm: 0.3651987910270691\n",
            "Epoch 75, Accuracy: 5.333333333333333%\n",
            "Epoch 76, Total Loss: 187220.234375, Classification Loss: 4.13914680480957, Reconstruction Loss: 187216.09375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 953175.75\n",
            "encoder.0.bias gradient norm: 32820.89453125\n",
            "decoder.0.weight gradient norm: 755098.875\n",
            "decoder.0.bias gradient norm: 28368.556640625\n",
            "decoder.2.weight gradient norm: 125036.171875\n",
            "decoder.2.bias gradient norm: 13965.787109375\n",
            "classifier.0.weight gradient norm: 2.6937501430511475\n",
            "classifier.0.bias gradient norm: 0.09310254454612732\n",
            "classifier.2.weight gradient norm: 1.180737853050232\n",
            "classifier.2.bias gradient norm: 0.051821548491716385\n",
            "classifier.4.weight gradient norm: 0.801476776599884\n",
            "classifier.4.bias gradient norm: 0.05652983486652374\n",
            "classifier.6.weight gradient norm: 0.8087363839149475\n",
            "classifier.6.bias gradient norm: 0.07328762114048004\n",
            "Epoch 76, Accuracy: 5.111111111111111%\n",
            "Epoch 77, Total Loss: 226026.84375, Classification Loss: 4.695935249328613, Reconstruction Loss: 226022.140625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1081715.25\n",
            "encoder.0.bias gradient norm: 39188.90625\n",
            "decoder.0.weight gradient norm: 1046551.6875\n",
            "decoder.0.bias gradient norm: 32649.236328125\n",
            "decoder.2.weight gradient norm: 264737.59375\n",
            "decoder.2.bias gradient norm: 16967.45703125\n",
            "classifier.0.weight gradient norm: 6.617147922515869\n",
            "classifier.0.bias gradient norm: 0.2191731035709381\n",
            "classifier.2.weight gradient norm: 2.989814043045044\n",
            "classifier.2.bias gradient norm: 0.11424826085567474\n",
            "classifier.4.weight gradient norm: 2.460084915161133\n",
            "classifier.4.bias gradient norm: 0.10605084896087646\n",
            "classifier.6.weight gradient norm: 3.8680460453033447\n",
            "classifier.6.bias gradient norm: 0.1652091145515442\n",
            "Epoch 77, Accuracy: 3.7777777777777777%\n",
            "Epoch 78, Total Loss: 207697.265625, Classification Loss: 4.599737644195557, Reconstruction Loss: 207692.671875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 933494.3125\n",
            "encoder.0.bias gradient norm: 31963.615234375\n",
            "decoder.0.weight gradient norm: 846435.25\n",
            "decoder.0.bias gradient norm: 30342.091796875\n",
            "decoder.2.weight gradient norm: 159758.8125\n",
            "decoder.2.bias gradient norm: 14611.033203125\n",
            "classifier.0.weight gradient norm: 5.023703575134277\n",
            "classifier.0.bias gradient norm: 0.1902686357498169\n",
            "classifier.2.weight gradient norm: 2.1361374855041504\n",
            "classifier.2.bias gradient norm: 0.09655818343162537\n",
            "classifier.4.weight gradient norm: 1.901911973953247\n",
            "classifier.4.bias gradient norm: 0.10272232443094254\n",
            "classifier.6.weight gradient norm: 2.5522546768188477\n",
            "classifier.6.bias gradient norm: 0.1467776894569397\n",
            "Epoch 78, Accuracy: 4.888888888888889%\n",
            "Epoch 79, Total Loss: 191883.796875, Classification Loss: 4.695123195648193, Reconstruction Loss: 191879.109375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 886027.875\n",
            "encoder.0.bias gradient norm: 32195.81640625\n",
            "decoder.0.weight gradient norm: 851946.0\n",
            "decoder.0.bias gradient norm: 27980.4609375\n",
            "decoder.2.weight gradient norm: 216623.875\n",
            "decoder.2.bias gradient norm: 15092.3759765625\n",
            "classifier.0.weight gradient norm: 6.605815887451172\n",
            "classifier.0.bias gradient norm: 0.21649862825870514\n",
            "classifier.2.weight gradient norm: 2.826672315597534\n",
            "classifier.2.bias gradient norm: 0.11121594160795212\n",
            "classifier.4.weight gradient norm: 2.25107479095459\n",
            "classifier.4.bias gradient norm: 0.10760818421840668\n",
            "classifier.6.weight gradient norm: 3.3446297645568848\n",
            "classifier.6.bias gradient norm: 0.16319257020950317\n",
            "Epoch 79, Accuracy: 3.5555555555555554%\n",
            "Epoch 80, Total Loss: 175352.6875, Classification Loss: 4.343746662139893, Reconstruction Loss: 175348.34375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 819579.75\n",
            "encoder.0.bias gradient norm: 27978.69921875\n",
            "decoder.0.weight gradient norm: 734008.5\n",
            "decoder.0.bias gradient norm: 27345.876953125\n",
            "decoder.2.weight gradient norm: 131682.359375\n",
            "decoder.2.bias gradient norm: 13326.9072265625\n",
            "classifier.0.weight gradient norm: 3.3805534839630127\n",
            "classifier.0.bias gradient norm: 0.14013440907001495\n",
            "classifier.2.weight gradient norm: 1.4795626401901245\n",
            "classifier.2.bias gradient norm: 0.06850405782461166\n",
            "classifier.4.weight gradient norm: 1.4699089527130127\n",
            "classifier.4.bias gradient norm: 0.0750790685415268\n",
            "classifier.6.weight gradient norm: 2.0391058921813965\n",
            "classifier.6.bias gradient norm: 0.10894504934549332\n",
            "Epoch 80, Accuracy: 6.222222222222222%\n",
            "Epoch 81, Total Loss: 197766.90625, Classification Loss: 4.1382551193237305, Reconstruction Loss: 197762.765625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 961374.125\n",
            "encoder.0.bias gradient norm: 35543.70703125\n",
            "decoder.0.weight gradient norm: 984355.0\n",
            "decoder.0.bias gradient norm: 32385.341796875\n",
            "decoder.2.weight gradient norm: 220105.046875\n",
            "decoder.2.bias gradient norm: 15627.4326171875\n",
            "classifier.0.weight gradient norm: 4.582106590270996\n",
            "classifier.0.bias gradient norm: 0.14614547789096832\n",
            "classifier.2.weight gradient norm: 1.9695746898651123\n",
            "classifier.2.bias gradient norm: 0.07788757234811783\n",
            "classifier.4.weight gradient norm: 1.3869311809539795\n",
            "classifier.4.bias gradient norm: 0.07811308652162552\n",
            "classifier.6.weight gradient norm: 1.8735604286193848\n",
            "classifier.6.bias gradient norm: 0.11503136903047562\n",
            "Epoch 81, Accuracy: 9.333333333333334%\n",
            "Epoch 82, Total Loss: 249770.984375, Classification Loss: 3.7803897857666016, Reconstruction Loss: 249767.203125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1366394.0\n",
            "encoder.0.bias gradient norm: 48128.66015625\n",
            "decoder.0.weight gradient norm: 1043508.9375\n",
            "decoder.0.bias gradient norm: 38782.390625\n",
            "decoder.2.weight gradient norm: 151782.453125\n",
            "decoder.2.bias gradient norm: 16491.9921875\n",
            "classifier.0.weight gradient norm: 1.662286639213562\n",
            "classifier.0.bias gradient norm: 0.056994978338479996\n",
            "classifier.2.weight gradient norm: 0.7389417886734009\n",
            "classifier.2.bias gradient norm: 0.031894613057374954\n",
            "classifier.4.weight gradient norm: 0.5378642082214355\n",
            "classifier.4.bias gradient norm: 0.03248586505651474\n",
            "classifier.6.weight gradient norm: 0.6685618758201599\n",
            "classifier.6.bias gradient norm: 0.04423324391245842\n",
            "Epoch 82, Accuracy: 5.111111111111111%\n",
            "Epoch 83, Total Loss: 220159.96875, Classification Loss: 5.103421211242676, Reconstruction Loss: 220154.859375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1069691.5\n",
            "encoder.0.bias gradient norm: 39457.75\n",
            "decoder.0.weight gradient norm: 1130313.375\n",
            "decoder.0.bias gradient norm: 35854.63671875\n",
            "decoder.2.weight gradient norm: 259846.5625\n",
            "decoder.2.bias gradient norm: 16968.4296875\n",
            "classifier.0.weight gradient norm: 8.879057884216309\n",
            "classifier.0.bias gradient norm: 0.297566294670105\n",
            "classifier.2.weight gradient norm: 3.6957523822784424\n",
            "classifier.2.bias gradient norm: 0.14456585049629211\n",
            "classifier.4.weight gradient norm: 3.1260337829589844\n",
            "classifier.4.bias gradient norm: 0.12806899845600128\n",
            "classifier.6.weight gradient norm: 5.103464603424072\n",
            "classifier.6.bias gradient norm: 0.192153662443161\n",
            "Epoch 83, Accuracy: 3.7777777777777777%\n",
            "Epoch 84, Total Loss: 203883.890625, Classification Loss: 4.584362030029297, Reconstruction Loss: 203879.3125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 921346.4375\n",
            "encoder.0.bias gradient norm: 31764.509765625\n",
            "decoder.0.weight gradient norm: 834634.6875\n",
            "decoder.0.bias gradient norm: 30414.935546875\n",
            "decoder.2.weight gradient norm: 145091.71875\n",
            "decoder.2.bias gradient norm: 14760.6025390625\n",
            "classifier.0.weight gradient norm: 4.752918720245361\n",
            "classifier.0.bias gradient norm: 0.17798511683940887\n",
            "classifier.2.weight gradient norm: 2.1810462474823\n",
            "classifier.2.bias gradient norm: 0.09807674586772919\n",
            "classifier.4.weight gradient norm: 2.3617241382598877\n",
            "classifier.4.bias gradient norm: 0.10894526541233063\n",
            "classifier.6.weight gradient norm: 3.669727087020874\n",
            "classifier.6.bias gradient norm: 0.1630963385105133\n",
            "Epoch 84, Accuracy: 5.555555555555555%\n",
            "Epoch 85, Total Loss: 190469.921875, Classification Loss: 4.727730751037598, Reconstruction Loss: 190465.1875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 893750.6875\n",
            "encoder.0.bias gradient norm: 32808.33984375\n",
            "decoder.0.weight gradient norm: 918830.1875\n",
            "decoder.0.bias gradient norm: 30151.31640625\n",
            "decoder.2.weight gradient norm: 211316.984375\n",
            "decoder.2.bias gradient norm: 15237.1005859375\n",
            "classifier.0.weight gradient norm: 4.331771373748779\n",
            "classifier.0.bias gradient norm: 0.13877429068088531\n",
            "classifier.2.weight gradient norm: 2.0595688819885254\n",
            "classifier.2.bias gradient norm: 0.0769997164607048\n",
            "classifier.4.weight gradient norm: 2.4900944232940674\n",
            "classifier.4.bias gradient norm: 0.09241581708192825\n",
            "classifier.6.weight gradient norm: 3.2947280406951904\n",
            "classifier.6.bias gradient norm: 0.1317843794822693\n",
            "Epoch 85, Accuracy: 3.7777777777777777%\n",
            "Epoch 86, Total Loss: 170794.296875, Classification Loss: 4.9912309646606445, Reconstruction Loss: 170789.3125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 694031.5625\n",
            "encoder.0.bias gradient norm: 23683.15234375\n",
            "decoder.0.weight gradient norm: 667128.75\n",
            "decoder.0.bias gradient norm: 23304.326171875\n",
            "decoder.2.weight gradient norm: 135283.609375\n",
            "decoder.2.bias gradient norm: 13235.9970703125\n",
            "classifier.0.weight gradient norm: 6.629798412322998\n",
            "classifier.0.bias gradient norm: 0.2640224099159241\n",
            "classifier.2.weight gradient norm: 2.839649200439453\n",
            "classifier.2.bias gradient norm: 0.12652739882469177\n",
            "classifier.4.weight gradient norm: 3.274118661880493\n",
            "classifier.4.bias gradient norm: 0.14157450199127197\n",
            "classifier.6.weight gradient norm: 5.722325801849365\n",
            "classifier.6.bias gradient norm: 0.23591023683547974\n",
            "Epoch 86, Accuracy: 5.777777777777778%\n",
            "Epoch 87, Total Loss: 167717.1875, Classification Loss: 4.077159881591797, Reconstruction Loss: 167713.109375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 724801.875\n",
            "encoder.0.bias gradient norm: 26638.701171875\n",
            "decoder.0.weight gradient norm: 787386.1875\n",
            "decoder.0.bias gradient norm: 25495.34765625\n",
            "decoder.2.weight gradient norm: 184370.5\n",
            "decoder.2.bias gradient norm: 13747.7734375\n",
            "classifier.0.weight gradient norm: 3.260211944580078\n",
            "classifier.0.bias gradient norm: 0.1101108118891716\n",
            "classifier.2.weight gradient norm: 1.3096035718917847\n",
            "classifier.2.bias gradient norm: 0.05277061462402344\n",
            "classifier.4.weight gradient norm: 1.1890748739242554\n",
            "classifier.4.bias gradient norm: 0.05655454471707344\n",
            "classifier.6.weight gradient norm: 1.5772342681884766\n",
            "classifier.6.bias gradient norm: 0.08439131826162338\n",
            "Epoch 87, Accuracy: 4.666666666666667%\n",
            "Epoch 88, Total Loss: 171315.734375, Classification Loss: 4.11253023147583, Reconstruction Loss: 171311.625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 823640.8125\n",
            "encoder.0.bias gradient norm: 27894.0\n",
            "decoder.0.weight gradient norm: 771024.75\n",
            "decoder.0.bias gradient norm: 26090.8125\n",
            "decoder.2.weight gradient norm: 133472.90625\n",
            "decoder.2.bias gradient norm: 13196.5078125\n",
            "classifier.0.weight gradient norm: 3.340573787689209\n",
            "classifier.0.bias gradient norm: 0.10955515503883362\n",
            "classifier.2.weight gradient norm: 1.3572486639022827\n",
            "classifier.2.bias gradient norm: 0.05372603237628937\n",
            "classifier.4.weight gradient norm: 1.156855583190918\n",
            "classifier.4.bias gradient norm: 0.05872784182429314\n",
            "classifier.6.weight gradient norm: 1.5599169731140137\n",
            "classifier.6.bias gradient norm: 0.08643338829278946\n",
            "Epoch 88, Accuracy: 3.111111111111111%\n",
            "Epoch 89, Total Loss: 229456.84375, Classification Loss: 5.122012615203857, Reconstruction Loss: 229451.71875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1126080.25\n",
            "encoder.0.bias gradient norm: 41210.48828125\n",
            "decoder.0.weight gradient norm: 1195640.125\n",
            "decoder.0.bias gradient norm: 37405.84375\n",
            "decoder.2.weight gradient norm: 243869.875\n",
            "decoder.2.bias gradient norm: 17281.171875\n",
            "classifier.0.weight gradient norm: 6.695496082305908\n",
            "classifier.0.bias gradient norm: 0.22570835053920746\n",
            "classifier.2.weight gradient norm: 3.3163084983825684\n",
            "classifier.2.bias gradient norm: 0.126071035861969\n",
            "classifier.4.weight gradient norm: 3.5931832790374756\n",
            "classifier.4.bias gradient norm: 0.1376916468143463\n",
            "classifier.6.weight gradient norm: 6.1136884689331055\n",
            "classifier.6.bias gradient norm: 0.21686984598636627\n",
            "Epoch 89, Accuracy: 3.111111111111111%\n",
            "Epoch 90, Total Loss: 218685.828125, Classification Loss: 4.494993686676025, Reconstruction Loss: 218681.328125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1269691.25\n",
            "encoder.0.bias gradient norm: 43678.71875\n",
            "decoder.0.weight gradient norm: 1012629.3125\n",
            "decoder.0.bias gradient norm: 35236.0859375\n",
            "decoder.2.weight gradient norm: 150713.75\n",
            "decoder.2.bias gradient norm: 15228.03515625\n",
            "classifier.0.weight gradient norm: 5.131928443908691\n",
            "classifier.0.bias gradient norm: 0.18356885015964508\n",
            "classifier.2.weight gradient norm: 2.551077127456665\n",
            "classifier.2.bias gradient norm: 0.10273058712482452\n",
            "classifier.4.weight gradient norm: 2.372079610824585\n",
            "classifier.4.bias gradient norm: 0.11385728418827057\n",
            "classifier.6.weight gradient norm: 3.7338664531707764\n",
            "classifier.6.bias gradient norm: 0.1980305165052414\n",
            "Epoch 90, Accuracy: 7.555555555555555%\n",
            "Epoch 91, Total Loss: 246797.046875, Classification Loss: 4.1767191886901855, Reconstruction Loss: 246792.875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1227255.5\n",
            "encoder.0.bias gradient norm: 45055.1015625\n",
            "decoder.0.weight gradient norm: 1319447.25\n",
            "decoder.0.bias gradient norm: 40755.23828125\n",
            "decoder.2.weight gradient norm: 273399.875\n",
            "decoder.2.bias gradient norm: 18309.84375\n",
            "classifier.0.weight gradient norm: 2.8840346336364746\n",
            "classifier.0.bias gradient norm: 0.09968220442533493\n",
            "classifier.2.weight gradient norm: 1.3301324844360352\n",
            "classifier.2.bias gradient norm: 0.05352690815925598\n",
            "classifier.4.weight gradient norm: 1.2052655220031738\n",
            "classifier.4.bias gradient norm: 0.0583808533847332\n",
            "classifier.6.weight gradient norm: 1.6246874332427979\n",
            "classifier.6.bias gradient norm: 0.08645089715719223\n",
            "Epoch 91, Accuracy: 4.444444444444445%\n",
            "Epoch 92, Total Loss: 213166.078125, Classification Loss: 4.412632465362549, Reconstruction Loss: 213161.671875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1230509.375\n",
            "encoder.0.bias gradient norm: 42830.171875\n",
            "decoder.0.weight gradient norm: 984204.0\n",
            "decoder.0.bias gradient norm: 34926.4296875\n",
            "decoder.2.weight gradient norm: 138497.328125\n",
            "decoder.2.bias gradient norm: 15126.4482421875\n",
            "classifier.0.weight gradient norm: 4.307846546173096\n",
            "classifier.0.bias gradient norm: 0.15202106535434723\n",
            "classifier.2.weight gradient norm: 1.8222218751907349\n",
            "classifier.2.bias gradient norm: 0.07668158411979675\n",
            "classifier.4.weight gradient norm: 1.8465543985366821\n",
            "classifier.4.bias gradient norm: 0.0939037948846817\n",
            "classifier.6.weight gradient norm: 2.4916155338287354\n",
            "classifier.6.bias gradient norm: 0.13782984018325806\n",
            "Epoch 92, Accuracy: 2.6666666666666665%\n",
            "Epoch 93, Total Loss: 208768.953125, Classification Loss: 4.9432573318481445, Reconstruction Loss: 208764.015625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 956779.1875\n",
            "encoder.0.bias gradient norm: 35331.93359375\n",
            "decoder.0.weight gradient norm: 1122095.375\n",
            "decoder.0.bias gradient norm: 34547.7109375\n",
            "decoder.2.weight gradient norm: 236344.25\n",
            "decoder.2.bias gradient norm: 16406.552734375\n",
            "classifier.0.weight gradient norm: 7.385860919952393\n",
            "classifier.0.bias gradient norm: 0.2350744754076004\n",
            "classifier.2.weight gradient norm: 3.3323471546173096\n",
            "classifier.2.bias gradient norm: 0.12477875500917435\n",
            "classifier.4.weight gradient norm: 3.9623684883117676\n",
            "classifier.4.bias gradient norm: 0.1500319391489029\n",
            "classifier.6.weight gradient norm: 6.279088973999023\n",
            "classifier.6.bias gradient norm: 0.25549158453941345\n",
            "Epoch 93, Accuracy: 6.444444444444445%\n",
            "Epoch 94, Total Loss: 189087.109375, Classification Loss: 4.176271915435791, Reconstruction Loss: 189082.9375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 928657.375\n",
            "encoder.0.bias gradient norm: 32229.953125\n",
            "decoder.0.weight gradient norm: 848736.5\n",
            "decoder.0.bias gradient norm: 30920.80078125\n",
            "decoder.2.weight gradient norm: 129036.296875\n",
            "decoder.2.bias gradient norm: 14101.9921875\n",
            "classifier.0.weight gradient norm: 3.6122639179229736\n",
            "classifier.0.bias gradient norm: 0.13411416113376617\n",
            "classifier.2.weight gradient norm: 1.5224641561508179\n",
            "classifier.2.bias gradient norm: 0.06667398661375046\n",
            "classifier.4.weight gradient norm: 1.4878454208374023\n",
            "classifier.4.bias gradient norm: 0.07134285569190979\n",
            "classifier.6.weight gradient norm: 1.918837308883667\n",
            "classifier.6.bias gradient norm: 0.09892910718917847\n",
            "Epoch 94, Accuracy: 5.555555555555555%\n",
            "Epoch 95, Total Loss: 201814.6875, Classification Loss: 4.807337284088135, Reconstruction Loss: 201809.875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 945135.6875\n",
            "encoder.0.bias gradient norm: 34677.07421875\n",
            "decoder.0.weight gradient norm: 1079746.875\n",
            "decoder.0.bias gradient norm: 33261.99609375\n",
            "decoder.2.weight gradient norm: 224230.265625\n",
            "decoder.2.bias gradient norm: 16001.7021484375\n",
            "classifier.0.weight gradient norm: 6.924246788024902\n",
            "classifier.0.bias gradient norm: 0.2225400060415268\n",
            "classifier.2.weight gradient norm: 2.9362802505493164\n",
            "classifier.2.bias gradient norm: 0.11024389415979385\n",
            "classifier.4.weight gradient norm: 3.077899694442749\n",
            "classifier.4.bias gradient norm: 0.1145901307463646\n",
            "classifier.6.weight gradient norm: 5.028011798858643\n",
            "classifier.6.bias gradient norm: 0.18797141313552856\n",
            "Epoch 95, Accuracy: 6.222222222222222%\n",
            "Epoch 96, Total Loss: 186189.453125, Classification Loss: 4.215211391448975, Reconstruction Loss: 186185.234375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 860124.625\n",
            "encoder.0.bias gradient norm: 29592.595703125\n",
            "decoder.0.weight gradient norm: 770351.125\n",
            "decoder.0.bias gradient norm: 28484.341796875\n",
            "decoder.2.weight gradient norm: 131436.3125\n",
            "decoder.2.bias gradient norm: 14001.0576171875\n",
            "classifier.0.weight gradient norm: 3.8523175716400146\n",
            "classifier.0.bias gradient norm: 0.15318997204303741\n",
            "classifier.2.weight gradient norm: 1.4479478597640991\n",
            "classifier.2.bias gradient norm: 0.0670221596956253\n",
            "classifier.4.weight gradient norm: 1.3940775394439697\n",
            "classifier.4.bias gradient norm: 0.06806469708681107\n",
            "classifier.6.weight gradient norm: 1.9150652885437012\n",
            "classifier.6.bias gradient norm: 0.0918717160820961\n",
            "Epoch 96, Accuracy: 4.222222222222222%\n",
            "Epoch 97, Total Loss: 184691.171875, Classification Loss: 4.539525508880615, Reconstruction Loss: 184686.625\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 846112.125\n",
            "encoder.0.bias gradient norm: 30420.482421875\n",
            "decoder.0.weight gradient norm: 837900.4375\n",
            "decoder.0.bias gradient norm: 27309.66796875\n",
            "decoder.2.weight gradient norm: 197567.3125\n",
            "decoder.2.bias gradient norm: 14724.8134765625\n",
            "classifier.0.weight gradient norm: 4.580035209655762\n",
            "classifier.0.bias gradient norm: 0.14763523638248444\n",
            "classifier.2.weight gradient norm: 1.9322007894515991\n",
            "classifier.2.bias gradient norm: 0.07546520978212357\n",
            "classifier.4.weight gradient norm: 2.0693440437316895\n",
            "classifier.4.bias gradient norm: 0.08958715945482254\n",
            "classifier.6.weight gradient norm: 3.0625662803649902\n",
            "classifier.6.bias gradient norm: 0.1441541612148285\n",
            "Epoch 97, Accuracy: 7.777777777777778%\n",
            "Epoch 98, Total Loss: 145231.40625, Classification Loss: 3.967951774597168, Reconstruction Loss: 145227.4375\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 667210.0\n",
            "encoder.0.bias gradient norm: 22907.947265625\n",
            "decoder.0.weight gradient norm: 548970.375\n",
            "decoder.0.bias gradient norm: 20935.330078125\n",
            "decoder.2.weight gradient norm: 112282.2109375\n",
            "decoder.2.bias gradient norm: 12009.9853515625\n",
            "classifier.0.weight gradient norm: 3.1751482486724854\n",
            "classifier.0.bias gradient norm: 0.13443072140216827\n",
            "classifier.2.weight gradient norm: 1.3844746351242065\n",
            "classifier.2.bias gradient norm: 0.06571713089942932\n",
            "classifier.4.weight gradient norm: 1.2615348100662231\n",
            "classifier.4.bias gradient norm: 0.06239406019449234\n",
            "classifier.6.weight gradient norm: 1.6311638355255127\n",
            "classifier.6.bias gradient norm: 0.08906108140945435\n",
            "Epoch 98, Accuracy: 2.6666666666666665%\n",
            "Epoch 99, Total Loss: 197110.546875, Classification Loss: 5.521346092224121, Reconstruction Loss: 197105.03125\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 949316.875\n",
            "encoder.0.bias gradient norm: 34476.1953125\n",
            "decoder.0.weight gradient norm: 872580.3125\n",
            "decoder.0.bias gradient norm: 28721.697265625\n",
            "decoder.2.weight gradient norm: 200994.765625\n",
            "decoder.2.bias gradient norm: 15438.9130859375\n",
            "classifier.0.weight gradient norm: 7.897696018218994\n",
            "classifier.0.bias gradient norm: 0.2617793679237366\n",
            "classifier.2.weight gradient norm: 3.692554473876953\n",
            "classifier.2.bias gradient norm: 0.14052259922027588\n",
            "classifier.4.weight gradient norm: 4.206856727600098\n",
            "classifier.4.bias gradient norm: 0.1489378809928894\n",
            "classifier.6.weight gradient norm: 6.312887668609619\n",
            "classifier.6.bias gradient norm: 0.22452904284000397\n",
            "Epoch 99, Accuracy: 2.2222222222222223%\n",
            "Epoch 100, Total Loss: 194486.90625, Classification Loss: 4.863450050354004, Reconstruction Loss: 194482.046875\n",
            "Learning rate: 0.001\n",
            "encoder.0.weight gradient norm: 1440491.5\n",
            "encoder.0.bias gradient norm: 50940.69140625\n",
            "decoder.0.weight gradient norm: 881671.9375\n",
            "decoder.0.bias gradient norm: 32775.61328125\n",
            "decoder.2.weight gradient norm: 123016.09375\n",
            "decoder.2.bias gradient norm: 14625.08984375\n",
            "classifier.0.weight gradient norm: 7.527270317077637\n",
            "classifier.0.bias gradient norm: 0.28795865178108215\n",
            "classifier.2.weight gradient norm: 3.618009567260742\n",
            "classifier.2.bias gradient norm: 0.15620820224285126\n",
            "classifier.4.weight gradient norm: 4.236440658569336\n",
            "classifier.4.bias gradient norm: 0.1761770397424698\n",
            "classifier.6.weight gradient norm: 7.010395050048828\n",
            "classifier.6.bias gradient norm: 0.3192775845527649\n",
            "Epoch 100, Accuracy: 6.0%\n"
          ]
        }
      ]
    }
  ]
}