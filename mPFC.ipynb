{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8ILzpyDUF+wjk6Judg2my",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SquirrelLover/FearNet-Implementation/blob/main/mPFC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50"
      ],
      "metadata": {
        "id": "NAa1Nl-k21SB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/davda54/sam.git\n",
        "!cp sam/sam.py ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rhcjebkYfnn",
        "outputId": "5c48bad1-39a0-4157-c8fe-4caf2856175a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sam'...\n",
            "remote: Enumerating objects: 200, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 200 (delta 85), reused 67 (delta 67), pack-reused 97\u001b[K\n",
            "Receiving objects: 100% (200/200), 657.08 KiB | 4.21 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAM optimizer\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults.update(self.base_optimizer.defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                self.state[p][\"old_p\"] = p.data.clone()\n",
        "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        super().load_state_dict(state_dict)\n",
        "        self.base_optimizer.param_groups = self.param_groups"
      ],
      "metadata": {
        "id": "fi9PsO8hZiaD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mini_batch = 450\n",
        "Dropout = 0.25\n",
        "Learning_Rate = 5e-4"
      ],
      "metadata": {
        "id": "0yTRBfMI1VgO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset (CIFAR-100)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR100(root='./data', train=True, transform = transform, download = True)\n",
        "test_data = torchvision.datasets.CIFAR100(root='./data', train=False, transform = transform, download = True)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = Mini_batch, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size = Mini_batch, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# view a couple of sample images to make sure they are loaded\n",
        "\n",
        "print(\"Train Loader Images:\")\n",
        "images, labels = next(iter(train_loader))\n",
        "images = images.cpu().detach().numpy()\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  plt.subplot(1, 4, i+1)\n",
        "  plt.imshow(np.transpose(images[i], (1, 2, 0)))\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"Image of class {labels[i].item()}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "gjJ6jJsagCvO",
        "outputId": "07f09f83-8cf0-4885-9856-13e853a194bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:04<00:00, 40422346.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Train Loader Images:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet processing\n",
        "\n",
        "resnet = resnet50(pretrained=True)\n",
        "resnet = resnet.to(device)\n",
        "resnet.eval()\n",
        "\n",
        "resnet_feature_extractor = nn.Sequential(*list(resnet.children())[:-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvJX8qO26DEb",
        "outputId": "291b4aa7-c621-46f1-f140-96bf7be33ebc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:02<00:00, 37.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass data through Resnet processing\n",
        "class CIFAR100FeatureDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        return image, label\n",
        "\n",
        "# training data\n",
        "num_images = len(train_data)\n",
        "feature_vectors = np.zeros((num_images, 2048))\n",
        "labels = np.zeros(num_images)\n",
        "\n",
        "feature_dataset = CIFAR100FeatureDataset(train_data)\n",
        "feature_loader = DataLoader(feature_dataset, batch_size=Mini_batch, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels_batch) in enumerate(feature_loader):\n",
        "        images = images.to(device)\n",
        "        features_batch = resnet_feature_extractor(images).squeeze().cpu().numpy()\n",
        "        start_index = i * Mini_batch\n",
        "        end_index = start_index + features_batch.shape[0]\n",
        "        feature_vectors[start_index:end_index] = features_batch\n",
        "        labels[start_index:end_index] = labels_batch.numpy()\n",
        "\n",
        "feature_vectors = torch.tensor(feature_vectors, dtype=torch.float32)\n",
        "labels = torch.tensor(labels).long()\n",
        "\n",
        "print(\"Training features shape:\", feature_vectors.shape)\n",
        "print(\"Training labels shape:\", labels.shape)\n",
        "\n",
        "# testing data\n",
        "num_images = len(test_data)\n",
        "test_feature_vectors = np.zeros((num_images, 2048))\n",
        "test_labels = np.zeros(num_images)\n",
        "\n",
        "test_feature_dataset = CIFAR100FeatureDataset(test_data)\n",
        "test_feature_loader = DataLoader(test_feature_dataset, batch_size=Mini_batch, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels_batch) in enumerate(test_feature_loader):\n",
        "        images = images.to(device)\n",
        "        features_batch = resnet_feature_extractor(images).squeeze().cpu().numpy()\n",
        "        start_index = i * Mini_batch\n",
        "        end_index = start_index + features_batch.shape[0]\n",
        "        test_feature_vectors[start_index:end_index] = features_batch\n",
        "        test_labels[start_index:end_index] = labels_batch.numpy()\n",
        "\n",
        "test_feature_vectors = torch.tensor(test_feature_vectors, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels).long()\n",
        "\n",
        "print(\"Test features shape:\", test_feature_vectors.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJZAPVHnKgwE",
        "outputId": "0df504ff-6aa7-46ce-911d-a1bc7ab6213e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features shape: torch.Size([50000, 2048])\n",
            "Training labels shape: torch.Size([50000])\n",
            "Test features shape: torch.Size([10000, 2048])\n",
            "Test labels shape: torch.Size([10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FUxN0nB320I6"
      },
      "outputs": [],
      "source": [
        "# define mPFC (long term)\n",
        "\n",
        "class mPFC(nn.Module):\n",
        "  def __init__(self, input_dim=2048, autoencoder_hidden_dims = np.array([1024, 512]), classifier_dims = np.array([1024, 512]), lambda_values=[1e4, 1.0, 0.1], learning_rate=5e-4):\n",
        "    super(mPFC, self).__init__()\n",
        "    # encoder\n",
        "    encoder_layers = []\n",
        "    current_dim = input_dim\n",
        "    encoder_layers.append(nn.ELU())\n",
        "    for hidden_dim in autoencoder_hidden_dims:\n",
        "      encoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "      encoder_layers.append(nn.ELU())\n",
        "      current_dim = hidden_dim\n",
        "    self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "    # decoder\n",
        "    decoder_layers = []\n",
        "    hidden_dims_reversed = list(autoencoder_hidden_dims[::-1])\n",
        "    for hidden_dim in hidden_dims_reversed:\n",
        "      decoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "      decoder_layers.append(nn.ELU())\n",
        "      current_dim = hidden_dim\n",
        "    decoder_layers.append(nn.Linear(current_dim, input_dim))\n",
        "    decoder_layers.append(nn.ELU())\n",
        "    self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    #classifier\n",
        "    current_dim = autoencoder_hidden_dims[-1]\n",
        "    classifier_layers= []\n",
        "\n",
        "    if len(classifier_dims) != 1:\n",
        "      classifier_layers.append(nn.Linear(current_dim, classifier_dims[0]))\n",
        "      classifier_layers.append(nn.ELU())\n",
        "      current_dim = classifier_dims[0]\n",
        "\n",
        "      for i in range(1, len(classifier_dims)-1):\n",
        "        hidden_dim = classifier_dims[i]\n",
        "        classifier_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "        classifier_layers.append(nn.ELU())\n",
        "        classifier_layers.append(nn.Dropout(p=Dropout))\n",
        "        current_dim = hidden_dim\n",
        "\n",
        "    classifier_layers.append(nn.Linear(current_dim, classifier_dims[len(classifier_dims)-1]))\n",
        "\n",
        "    self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    # lambda\n",
        "    self.lambda_values = torch.tensor(lambda_values if lambda_values else [1.0] * len(autoencoder_hidden_dims), dtype=torch.float32)\n",
        "\n",
        "    # optimizer\n",
        "    #self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    self.base_optimizer = optim.SGD\n",
        "    self.optimizer = SAM(self.parameters(), self.base_optimizer, rho=0.05, lr=learning_rate)\n",
        "\n",
        "  def encoder_forward(self, x):\n",
        "    encoder_intermediates = [x]\n",
        "    for layer in self.encoder:\n",
        "      x = layer(x)\n",
        "      encoder_intermediates.append(x)\n",
        "    encoded = encoder_intermediates[-1]\n",
        "    return encoded, encoder_intermediates\n",
        "\n",
        "  def decoder_forward(self, x):\n",
        "    decoder_intermediates = [x]\n",
        "    for layer in self.decoder:\n",
        "      x = layer(x)\n",
        "      decoder_intermediates.append(x)\n",
        "    pseudo_img = decoder_intermediates[-1]\n",
        "    return pseudo_img, list(decoder_intermediates[::-1])\n",
        "\n",
        "  def classify(self, x):\n",
        "    class_logits = self.classifier(x)\n",
        "    return class_logits\n",
        "\n",
        "  def compute_loss(self, class_logits, targets, encoder_intermediates, decoder_intermediates):\n",
        "    # classification loss\n",
        "    classification_loss = nn.CrossEntropyLoss()(class_logits, targets)\n",
        "\n",
        "    # reconstruction loss with lambda weighting\n",
        "    reconstruction_loss = 0\n",
        "    for i in range(len(self.lambda_values)):\n",
        "      encoder_hidden = encoder_intermediates[i]\n",
        "      decoder_hidden = decoder_intermediates[i]\n",
        "      diff = encoder_hidden - decoder_hidden\n",
        "      squared_diff = diff.pow(2)\n",
        "      layer_loss = squared_diff.sum()\n",
        "      reconstruction_loss += self.lambda_values[i] * layer_loss\n",
        "\n",
        "    # total loss\n",
        "    total_loss = classification_loss + reconstruction_loss\n",
        "\n",
        "    return classification_loss, reconstruction_loss, total_loss\n",
        "\n",
        "  def training_step(self, data_loader, device):\n",
        "    self.train()\n",
        "\n",
        "    total_classification_loss = 0\n",
        "    total_reconstruction_loss = 0\n",
        "    total_total_loss = 0\n",
        "\n",
        "    for x, targets in data_loader:\n",
        "      x, targets = x.to(device), targets.to(device)\n",
        "\n",
        "      # first forward-backward pass\n",
        "      # forward pass\n",
        "      encoded, encoder_intermediates = self.encoder_forward(x)\n",
        "      pseudo_img, decoder_intermediates = self.decoder_forward(encoded)\n",
        "      class_logits = self.classify(encoded)\n",
        "      # compute losses\n",
        "      classification_loss, reconstruction_loss, total_loss = self.compute_loss(\n",
        "        class_logits, targets, encoder_intermediates, decoder_intermediates\n",
        "      )\n",
        "\n",
        "      # zero gradients\n",
        "      self.optimizer.zero_grad()\n",
        "      for param in self.decoder.parameters():\n",
        "        param.grad = None\n",
        "      for param in self.encoder.parameters():\n",
        "        param.grad = None\n",
        "\n",
        "      # backward pass\n",
        "      classification_loss.backward(retain_graph=True)\n",
        "      classifier_grads = {name: param.grad.clone() for name, param in self.classifier.named_parameters()}\n",
        "      reconstruction_loss.backward(retain_graph=True)\n",
        "      decoder_grads = {name: param.grad.clone() for name, param in self.decoder.named_parameters()}\n",
        "      total_loss.backward()\n",
        "      for name, param in self.encoder.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = param.grad.data\n",
        "\n",
        "      # optimizer\n",
        "      #encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      encoder_optimizer = SAM(self.encoder.parameters(), self.base_optimizer, rho=0.05, lr=self.optimizer.defaults['lr'])\n",
        "      encoder_optimizer.step()\n",
        "      #classifier_optimizer = optim.Adam(self.classifier.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      classifier_optimizer = SAM(self.classifier.parameters(), self.base_optimizer, rho=0.05, lr=self.optimizer.defaults['lr'])\n",
        "      classifier_optimizer.step()\n",
        "      #decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      decoder_optimizer = SAM(self.decoder.parameters(), self.base_optimizer, rho=0.05, lr=self.optimizer.defaults['lr'])\n",
        "      decoder_optimizer.step()\n",
        "\n",
        "      # update parameters\n",
        "      for name, param in self.classifier.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = classifier_grads[name].data\n",
        "      for name, param in self.decoder.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = decoder_grads[name].data\n",
        "\n",
        "      # add losses to total\n",
        "      total_classification_loss += classification_loss.item()\n",
        "      total_reconstruction_loss += reconstruction_loss.item()\n",
        "      total_total_loss += total_loss.item()\n",
        "\n",
        "    # average loss\n",
        "    average_classification_loss = total_classification_loss / len(data_loader)\n",
        "    average_reconstruction_loss = total_reconstruction_loss / len(data_loader)\n",
        "    average_total_loss = total_total_loss / len(data_loader)\n",
        "\n",
        "    return average_classification_loss, average_reconstruction_loss, average_total_loss\n",
        "\n",
        "  def generate_statistics(self, list_feature_vectors, device):\n",
        "    self.eval()\n",
        "\n",
        "    latent_rep = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, targets in list_feature_vectors:\n",
        "        x, targets = x.to(device), targets.to(device)\n",
        "\n",
        "        # pass through encoder\n",
        "        mPFC_out, _ = self.encoder_forward(x)\n",
        "        latent_rep.append(mPFC_out.cpu())\n",
        "\n",
        "    # stack\n",
        "    latent_rep = torch.cat(latent_rep, dim=0)\n",
        "\n",
        "    # ux\n",
        "    ux_mPFC = latent_rep.mean(dim=0)\n",
        "\n",
        "    # covariance matrix\n",
        "    centered_mPFC_features = latent_rep - ux_mPFC\n",
        "    covariance_matrix_mPFC = torch.matmul(centered_mPFC_features.t(), centered_mPFC_features) / (latent_rep.size(0) - 1)\n",
        "\n",
        "    return ux_mPFC, covariance_matrix_mPFC\n",
        "\n",
        "  def pseudoimg_from_statistics(self, u, covar, count):\n",
        "    self.eval()\n",
        "\n",
        "    # sampling from a multivariate normal distribution\n",
        "    distribution = torch.distributions.MultivariateNormal(u, covar)\n",
        "    sampled_vectors = distribution.sample((count,)).to(device)\n",
        "\n",
        "    # pass through decoder\n",
        "    pseudo_images = []\n",
        "    with torch.no_grad():\n",
        "      for latent_vector in sampled_vectors:\n",
        "        pseudo_img, _ = self.decoder_forward(latent_vector)\n",
        "        pseudo_images.append(pseudo_img.cpu())\n",
        "\n",
        "    # stack\n",
        "    pseudo_images = torch.stack(pseudo_images, dim=0)\n",
        "\n",
        "    return pseudo_images\n",
        "\n",
        "  def calculate_accuracy(self, data_loader, device):\n",
        "    self.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, targets in data_loader:\n",
        "        x, targets = x.to(device), targets.to(device)\n",
        "        encoded, _ = self.encoder_forward(x)\n",
        "        class_logits = self.classify(encoded)\n",
        "        _, predicted = torch.max(class_logits.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = mPFC()\n",
        "model = model.to(device)\n",
        "\n",
        "#print(summary(model.encoder, input_size=(3072,)))\n",
        "#print(summary(model.decoder, input_size=(256,)))\n",
        "\n",
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels.long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "        return feature, label\n",
        "\n",
        "dataset = FeatureDataset(feature_vectors, labels)\n",
        "\n",
        "for epoch in range(30):\n",
        "    data_loader = DataLoader(dataset, batch_size=Mini_batch, shuffle=True)\n",
        "\n",
        "    avg_classification_loss, avg_reconstruction_loss, avg_total_loss = model.training_step(data_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Total Loss: {avg_total_loss}, Classification Loss: {avg_classification_loss}, Reconstruction Loss: {avg_reconstruction_loss}\")\n",
        "    #for param_group in model.optimizer.param_groups:\n",
        "    #    print(\"Learning rate:\", param_group['lr'])\n",
        "    #for name, param in model.named_parameters():\n",
        "    #    if param.grad is not None:\n",
        "    #        print(f\"{name} gradient norm: {param.grad.norm().item()}\")\n",
        "\n",
        "    accuracy = model.calculate_accuracy(data_loader, device)\n",
        "    print(f\"Epoch {epoch+1}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "BzxGXR78gedJ",
        "outputId": "d733a866-e5de-444d-8a64-822c4d246642"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects a non-empty TensorList",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-aae312849412>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mavg_classification_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reconstruction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_total_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}, Total Loss: {avg_total_loss}, Classification Loss: {avg_classification_loss}, Reconstruction Loss: {avg_reconstruction_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-5789125485f3>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, data_loader, device)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m       \u001b[0;31m# optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0mclassification_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m       \u001b[0;31m# add losses to total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-fda2582d303b>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# the closure should do a full forward-backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-fda2582d303b>\u001b[0m in \u001b[0;36mfirst_step\u001b[0;34m(self, zero_grad)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfirst_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rho\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-fda2582d303b>\u001b[0m in \u001b[0;36m_grad_norm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mshared_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m  \u001b[0;31m# put everything on the same device, in case of model parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         norm = torch.norm(\n\u001b[0;32m---> 54\u001b[0;31m                     torch.stack([\n\u001b[0m\u001b[1;32m     55\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"adaptive\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test accuracy\n",
        "\n",
        "test_dataset = FeatureDataset(test_feature_vectors, test_labels)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=len(test_labels), shuffle=True)\n",
        "\n",
        "avg_classification_loss, avg_reconstruction_loss, avg_total_loss = model.training_step(test_data_loader, device)\n",
        "\n",
        "print(f\"Total Loss: {avg_total_loss}, Classification Loss: {avg_classification_loss}, Reconstruction Loss: {avg_reconstruction_loss}\")\n",
        "\n",
        "accuracy = model.calculate_accuracy(test_data_loader, device)\n",
        "print(f\"Accuracy: {accuracy}%\")\n"
      ],
      "metadata": {
        "id": "PcBDBR8LLNb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}