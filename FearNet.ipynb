{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj2K6Qs9s9bn"
      },
      "source": [
        "Implementation of a Fearnet Variation\n",
        "\n",
        "Authors: Brady Gho, Lucy Wu\n",
        "\n",
        "Source: https://arxiv.org/abs/1711.10563\n",
        "\n",
        "Created on July 25, 2024. Last edited July 25, 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxb1oT_wLvLv"
      },
      "source": [
        "# Defining Imports and Meta-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ir8RTlFR2G9X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hR2-6d6qtZY6"
      },
      "outputs": [],
      "source": [
        "# Define meta-Parameters\n",
        "Mini_batch = 450\n",
        "Dropout = 0.25\n",
        "VAE_dims = np.array([256])\n",
        "Classifier_Dims = np.array([961, 525, 289, 100])\n",
        "Input_Dim = 2048\n",
        "Learning_Rate = 2e-3\n",
        "HC_Epochs = 25\n",
        "mPFC_Epochs = 35\n",
        "BLA_Epochs = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdVa6JvuLHcT"
      },
      "source": [
        "# Loading and Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "8xFkEuoHs45K",
        "outputId": "dc6eb0c9-3f2d-4ebb-a7dc-d696300f4f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:01<00:00, 93225200.20it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Train Loader Images:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load Dataset (CIFAR-100)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # insert more transformations of data i.e. normalization\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR100(root='./data', train=True, transform = transform, download = True)\n",
        "test_data = torchvision.datasets.CIFAR100(root='./data', train=False, transform = transform, download = True)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = Mini_batch, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size = Mini_batch, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# view a couple of sample images to make sure they are loaded\n",
        "\n",
        "print(\"Train Loader Images:\")\n",
        "images, labels = next(iter(train_loader))\n",
        "images = images.cpu().detach().numpy()\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  plt.subplot(1, 4, i+1)\n",
        "  plt.imshow(np.transpose(images[i], (1, 2, 0)))\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"Image of class {labels[i].item()}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DwgeK1JaxNaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02ee0bf-49f7-41e7-f2c2-88a51a2608c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 198MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Resnet Processing\n",
        "\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "resnet = resnet50(pretrained=True)\n",
        "resnet = resnet.to(device)\n",
        "resnet.eval()\n",
        "\n",
        "# remove all fc layers except the last one\n",
        "resnet_feature_extractor = nn.Sequential(*list(resnet.children())[:-1]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r__2Oki62-8a",
        "outputId": "9519f5fa-f766-4082-a2ae-7fd6e0bbfdcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-876542bac5b9>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  feature_vectors = torch.tensor(feature_vectors, dtype=torch.float32).to(device)\n",
            "<ipython-input-16-876542bac5b9>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).long().to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features shape: torch.Size([50000, 2048])\n",
            "Training labels shape: torch.Size([50000])\n",
            "Test features shape: torch.Size([10000, 2048])\n",
            "Test labels shape: torch.Size([10000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-876542bac5b9>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_feature_vectors = torch.tensor(test_feature_vectors, dtype=torch.float32).to(device)\n",
            "<ipython-input-16-876542bac5b9>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_labels = torch.tensor(test_labels).long().to(device)\n"
          ]
        }
      ],
      "source": [
        "# pass data through Resnet processing\n",
        "class CIFAR100FeatureDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        return image, label\n",
        "\n",
        "# training data\n",
        "num_images = len(train_data)\n",
        "feature_vectors = torch.zeros((num_images, 2048), device=device)\n",
        "labels = torch.zeros(num_images, device=device)\n",
        "\n",
        "feature_dataset = CIFAR100FeatureDataset(train_data)\n",
        "feature_loader = DataLoader(feature_dataset, batch_size=Mini_batch, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels_batch) in enumerate(feature_loader):\n",
        "        images = images.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "        features_batch = resnet_feature_extractor(images).squeeze()\n",
        "        start_index = i * Mini_batch\n",
        "        end_index = start_index + features_batch.shape[0]\n",
        "        feature_vectors[start_index:end_index] = features_batch\n",
        "        labels[start_index:end_index] = labels_batch\n",
        "\n",
        "feature_vectors = torch.tensor(feature_vectors, dtype=torch.float32).to(device)\n",
        "labels = torch.tensor(labels).long().to(device)\n",
        "\n",
        "print(\"Training features shape:\", feature_vectors.shape)\n",
        "print(\"Training labels shape:\", labels.shape)\n",
        "\n",
        "# testing data\n",
        "num_images = len(test_data)\n",
        "test_feature_vectors = torch.zeros((num_images, 2048), device=device)\n",
        "test_labels = torch.zeros(num_images, device=device)\n",
        "\n",
        "test_feature_dataset = CIFAR100FeatureDataset(test_data)\n",
        "test_feature_loader = DataLoader(test_feature_dataset, batch_size=Mini_batch, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels_batch) in enumerate(test_feature_loader):\n",
        "        images = images.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "        features_batch = resnet_feature_extractor(images).squeeze()\n",
        "        start_index = i * Mini_batch\n",
        "        end_index = start_index + features_batch.shape[0]\n",
        "        test_feature_vectors[start_index:end_index] = features_batch\n",
        "        test_labels[start_index:end_index] = labels_batch\n",
        "\n",
        "test_feature_vectors = torch.tensor(test_feature_vectors, dtype=torch.float32).to(device)\n",
        "test_labels = torch.tensor(test_labels).long().to(device)\n",
        "\n",
        "print(\"Test features shape:\", test_feature_vectors.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YDsBDqLCM6EX"
      },
      "outputs": [],
      "source": [
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels.long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "        return feature, label\n",
        "\n",
        "dataset = FeatureDataset(feature_vectors, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GPXUTBkygJd"
      },
      "source": [
        "# Evaluate the Performance of Resnet on CIFAR-100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hzMmn2s6yn3z",
        "outputId": "7920f108-e2ea-44c0-d159-bd5092406b9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "resnet_test = resnet50(pretrained=True)\n",
        "num_features = resnet_test.fc.in_features\n",
        "resnet_test.fc = nn.Linear(num_features, 100)\n",
        "resnet_test.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIhPefE0zHBV"
      },
      "outputs": [],
      "source": [
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet_test.parameters(), lr=0.001)\n",
        "\n",
        "# training function\n",
        "def training_step(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    average_loss = total_loss / len(data_loader)\n",
        "    return average_loss\n",
        "\n",
        "# calculate accuracy\n",
        "def calculate_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# training\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    avg_loss = training_step(resnet_test, train_loader, criterion, optimizer, device)\n",
        "    accuracy = calculate_accuracy(resnet_test, test_loader, device)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss}, Accuracy: {accuracy}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yPzQeiNwYdF",
        "outputId": "7fd72f48-ef77-455c-d46d-2b2d788055ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 57.72%\n"
          ]
        }
      ],
      "source": [
        "# test accuracy\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = evaluate_model(resnet_test, test_loader, device)\n",
        "print(f'Test Accuracy of the model on the 10000 test images: {test_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfYC56dhLO8u"
      },
      "source": [
        "# Hippocampus Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BLWq-gNEtX_6"
      },
      "outputs": [],
      "source": [
        "# define HC\n",
        "\n",
        "class HC(nn.Module):\n",
        "  def __init__(self, input_dim=Input_Dim, classifier_dims = Classifier_Dims, learning_rate= Learning_Rate, p_dropout=Dropout):\n",
        "    super(HC, self).__init__()\n",
        "\n",
        "    self.short_term_memory = []\n",
        "\n",
        "    classifier_layers = []\n",
        "    current_dim = input_dim\n",
        "\n",
        "    if len(classifier_dims)!=1:\n",
        "      # Avoid dropping out the first layer of processing and output layer\n",
        "      classifier_layers.append(nn.Linear(current_dim, classifier_dims[0]))\n",
        "      classifier_layers.append(nn.ReLU())\n",
        "\n",
        "      for i in range(1, len(classifier_dims)-1):\n",
        "        hidden_dim = classifier_dims[i]\n",
        "        classifier_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "        classifier_layers.append(nn.ReLU())\n",
        "        classifier_layers.append(nn.Dropout(p=p_dropout))\n",
        "        current_dim = hidden_dim\n",
        "\n",
        "    classifier_layers.append(nn.Linear(current_dim, classifier_dims[len(classifier_dims)-1]))\n",
        "\n",
        "    self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    # Using Adam Optimizer\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.short_term_memory.append(x)\n",
        "    class_logits = self.classifier(x)\n",
        "    return class_logits\n",
        "\n",
        "  def clear_memory(self):\n",
        "    memory = self.short_term_memory\n",
        "    self.short_term_memory = []\n",
        "    return memory\n",
        "\n",
        "  def training_step(self, dataloader):\n",
        "    total_loss = 0\n",
        "\n",
        "    self.train()\n",
        "    for features, targets in dataloader:\n",
        "      # classification loss\n",
        "      optimizer = self.optimizer\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      class_logits = self.forward(features)\n",
        "      loss = nn.CrossEntropyLoss()(class_logits, targets)\n",
        "\n",
        "      loss.backwards()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss += total_loss\n",
        "\n",
        "    return (total_loss / (len(dataloader) * dataloader.batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hippocampus = HC()\n",
        "\n",
        "loss_history = []\n",
        "for epoch in range(25):\n",
        "  loss = hippocampus.training_step(data_loader)\n",
        "  loss_history.append(loss)\n",
        "  print(f\"Loss at epoch {epoch+1}: {loss}\")\n",
        "\n",
        "plt.plot(loss_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "GKPmSaeDjyHn",
        "outputId": "d64c4038-94cb-4210-e37b-920cf695708b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-066e129bbcf8>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhippocampus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss at epoch {epoch+1}: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iAOTMobLSnf"
      },
      "source": [
        "# Pre-frontal Cortex  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9213wTvFtRdp"
      },
      "outputs": [],
      "source": [
        "# define mPFC (long term)\n",
        "\n",
        "class mPFC(nn.Module):\n",
        "  def __init__(self, input_dim=Input_Dim, autoencoder_hidden_dims = np.array([1024, 512]), classifier_dims = np.array([100]), lambda_values=[1e4, 1, 0.1], learning_rate=5e-4):\n",
        "    super(mPFC, self).__init__()\n",
        "\n",
        "    # encoder\n",
        "    encoder_layers = []\n",
        "    current_dim = input_dim\n",
        "    encoder_layers.append(nn.ELU())\n",
        "    for hidden_dim in autoencoder_hidden_dims:\n",
        "      encoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "      encoder_layers.append(nn.ELU())\n",
        "      current_dim = hidden_dim\n",
        "    self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "    # decoder\n",
        "    decoder_layers = []\n",
        "    hidden_dims_reversed = list(autoencoder_hidden_dims[::-1])\n",
        "    for hidden_dim in hidden_dims_reversed:\n",
        "      decoder_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "      decoder_layers.append(nn.ELU())\n",
        "      current_dim = hidden_dim\n",
        "    decoder_layers.append(nn.Linear(current_dim, input_dim))\n",
        "    decoder_layers.append(nn.ELU())\n",
        "    self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    #classifier\n",
        "    current_dim = autoencoder_hidden_dims[-1]\n",
        "    classifier_layers= []\n",
        "\n",
        "    if len(classifier_dims) != 1:\n",
        "      classifier_layers.append(nn.Linear(current_dim, classifier_dims[0]))\n",
        "      classifier_layers.append(nn.ELU())\n",
        "      current_dim = classifier_dims[0]\n",
        "\n",
        "      for i in range(1, len(classifier_dims)-1):\n",
        "        hidden_dim = classifier_dims[i]\n",
        "        classifier_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "        classifier_layers.append(nn.ELU())\n",
        "        classifier_layers.append(nn.Dropout(p=Dropout))\n",
        "        current_dim = hidden_dim\n",
        "\n",
        "    classifier_layers.append(nn.Linear(current_dim, classifier_dims[len(classifier_dims)-1]))\n",
        "\n",
        "    self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    # lambda\n",
        "    self.lambda_values = torch.tensor(lambda_values if lambda_values else [1.0] * len(autoencoder_hidden_dims), dtype=torch.float32)\n",
        "\n",
        "    # optimizer\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "  def encoder_forward(self, x):\n",
        "    encoder_intermediates = [x]\n",
        "    for layer in self.encoder:\n",
        "      x = layer(x)\n",
        "      encoder_intermediates.append(x)\n",
        "    encoded = encoder_intermediates[-1]\n",
        "    return encoded, encoder_intermediates\n",
        "\n",
        "  def decoder_forward(self, x):\n",
        "    decoder_intermediates = [x]\n",
        "    for layer in self.decoder:\n",
        "      x = layer(x)\n",
        "      decoder_intermediates.append(x)\n",
        "    pseudo_img = decoder_intermediates[-1]\n",
        "    return pseudo_img, list(decoder_intermediates[::-1])\n",
        "\n",
        "  def classify(self, x):\n",
        "    class_logits = self.classifier(x)\n",
        "    return class_logits\n",
        "\n",
        "  def compute_loss(self, class_logits, targets, encoder_intermediates, decoder_intermediates):\n",
        "\n",
        "    # classification loss\n",
        "    classification_loss = nn.CrossEntropyLoss()(class_logits, targets)\n",
        "\n",
        "    # reconstruction loss with lambda weighting\n",
        "    reconstruction_loss = 0\n",
        "    for i in range(len(self.lambda_values)):\n",
        "      encoder_hidden = encoder_intermediates[i]\n",
        "      decoder_hidden = decoder_intermediates[i]\n",
        "      diff = encoder_hidden - decoder_hidden\n",
        "      squared_diff = diff.pow(2)\n",
        "      layer_loss = squared_diff.sum()\n",
        "      reconstruction_loss += self.lambda_values[i] * layer_loss\n",
        "\n",
        "    # total loss\n",
        "    total_loss = classification_loss + reconstruction_loss\n",
        "\n",
        "    return classification_loss, reconstruction_loss, total_loss\n",
        "\n",
        "  def training_step(self, data_loader, device):\n",
        "    self.train()\n",
        "\n",
        "    total_classification_loss = 0\n",
        "    total_reconstruction_loss = 0\n",
        "    total_total_loss = 0\n",
        "\n",
        "    for x, targets in data_loader:\n",
        "      x, targets = x.to(device), targets.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      encoded, encoder_intermediates = self.encoder_forward(x)\n",
        "      pseudo_img, decoder_intermediates = self.decoder_forward(encoded)\n",
        "      class_logits = self.classify(encoded)\n",
        "\n",
        "      # compute losses\n",
        "      classification_loss, reconstruction_loss, total_loss = self.compute_loss(\n",
        "        class_logits, targets, encoder_intermediates, decoder_intermediates\n",
        "      )\n",
        "\n",
        "      # zero gradients\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      # backward pass\n",
        "      classification_loss.backward(retain_graph=True)\n",
        "      classifier_grads = {name: param.grad.clone() for name, param in self.classifier.named_parameters()}\n",
        "      reconstruction_loss.backward(retain_graph=True)\n",
        "      decoder_grads = {name: param.grad.clone() for name, param in self.decoder.named_parameters()}\n",
        "      total_loss.backward()\n",
        "      encoder_grads = {name: param.grad.clone() for name, param in self.encoder.named_parameters()}\n",
        "\n",
        "      # optimizer\n",
        "      encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      encoder_optimizer.step()\n",
        "      classifier_optimizer = optim.Adam(self.classifier.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      classifier_optimizer.step()\n",
        "      decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=self.optimizer.defaults['lr'])\n",
        "      decoder_optimizer.step()\n",
        "\n",
        "      # update parameters\n",
        "      for name, param in self.classifier.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = classifier_grads[name].data\n",
        "      for name, param in self.decoder.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = decoder_grads[name].data\n",
        "      for name, param in self.encoder.named_parameters():\n",
        "        if param.grad is not None:\n",
        "          param.grad.data = encoder_grads[name].data\n",
        "\n",
        "      # add losses to total\n",
        "      total_classification_loss += classification_loss.item()\n",
        "      total_reconstruction_loss += reconstruction_loss.item()\n",
        "      total_total_loss += total_loss.item()\n",
        "\n",
        "    # average loss\n",
        "    average_classification_loss = total_classification_loss / len(data_loader)\n",
        "    average_reconstruction_loss = total_reconstruction_loss / len(data_loader)\n",
        "    average_total_loss = total_total_loss / len(data_loader)\n",
        "\n",
        "    return average_classification_loss, average_reconstruction_loss, average_total_loss\n",
        "\n",
        "  def generate_statistics(self, list_feature_vectors, device):\n",
        "    self.eval()\n",
        "\n",
        "    latent_rep = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, targets in list_feature_vectors:\n",
        "        x, targets = x.to(device), targets.to(device)\n",
        "\n",
        "        # pass through encoder\n",
        "        mPFC_out, _ = self.encoder_forward(x)\n",
        "        latent_rep.append(mPFC_out.cpu())\n",
        "\n",
        "    # stack\n",
        "    latent_rep = torch.cat(latent_rep, dim=0)\n",
        "\n",
        "    # ux\n",
        "    ux_mPFC = latent_rep.mean(dim=0)\n",
        "\n",
        "    # covariance matrix\n",
        "    centered_mPFC_features = latent_rep - ux_mPFC\n",
        "    covariance_matrix_mPFC = torch.matmul(centered_mPFC_features.t(), centered_mPFC_features) / (latent_rep.size(0) - 1)\n",
        "\n",
        "    return ux_mPFC, covariance_matrix_mPFC\n",
        "\n",
        "  def pseudoimg_from_statistics(self, u, covar, count):\n",
        "    self.eval()\n",
        "\n",
        "    # sampling from a multivariate normal distribution\n",
        "    distribution = torch.distributions.MultivariateNormal(u, covar)\n",
        "    sampled_vectors = distribution.sample((count,)).to(device)\n",
        "\n",
        "    # pass through decoder\n",
        "    pseudo_images = []\n",
        "    with torch.no_grad():\n",
        "      for latent_vector in sampled_vectors:\n",
        "        pseudo_img, _ = self.decoder_forward(latent_vector)\n",
        "        pseudo_images.append(pseudo_img.cpu())\n",
        "\n",
        "    # stack\n",
        "    pseudo_images = torch.stack(pseudo_images, dim=0)\n",
        "\n",
        "    return pseudo_images\n",
        "\n",
        "  def calculate_accuracy(self, data_loader, device):\n",
        "    self.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, targets in data_loader:\n",
        "        x, targets = x.to(device), targets.to(device)\n",
        "        encoded, _ = self.encoder_forward(x)\n",
        "        class_logits = self.classify(encoded)\n",
        "        _, predicted = torch.max(class_logits.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKg46QH1Lefr"
      },
      "source": [
        "# BLA Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JGy_SZT9tdgY"
      },
      "outputs": [],
      "source": [
        "# Define BLA\n",
        "\n",
        "class BLA(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim=Input_Dim, classifier_dims = Classifier_Dims, learning_rate= Learning_Rate, p_dropout=Dropout):\n",
        "    super(BLA, self).__init__()\n",
        "\n",
        "    classifier_layers = []\n",
        "    current_dim = input_dim\n",
        "\n",
        "    if len(classifier_dims)!=1:\n",
        "      # Avoid dropping out the first layer of processing and output layer\n",
        "      classifier_layers.append(nn.Linear(current_dim, classifier_dims[0]))\n",
        "      classifier_layers.append(nn.ReLU())\n",
        "\n",
        "      for i in range(1, len(classifier_dims)):\n",
        "        hidden_dim = classifier_dims[i]\n",
        "        classifier_layers.append(nn.Linear(current_dim, hidden_dim))\n",
        "        classifier_layers.append(nn.ReLU())\n",
        "        classifier_layers.append(nn.Dropout(p=p_dropout))\n",
        "        current_dim = hidden_dim\n",
        "\n",
        "    classifier_layers.append(nn.Linear(current_dim, 1))\n",
        "\n",
        "    self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    # Using Adam Optimizer\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    class_logits = self.classifier(x)\n",
        "    return class_logits\n",
        "\n",
        "  def training_step(self, dataloader, HC, mPFC):\n",
        "    total_loss = 0\n",
        "\n",
        "    self.train()\n",
        "    HC.eval()\n",
        "    mPFC.eval()\n",
        "    for features, label in dataloader:\n",
        "      HC_logits = []\n",
        "      mPFC_logits = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "        HC_logits = HC.forward(features)\n",
        "        mPFC_logits = mPFC.forward(features)\n",
        "\n",
        "      optimizer = self.optimizer\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      class_prob = self.Softmax(self.forward(features))\n",
        "\n",
        "      # class_prob =\n",
        "\n",
        "      HC_prob = nn.Softmax(HC_logits)\n",
        "      mPFC_prob = nn.Softmax(mPFC_logits)\n",
        "\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "      # <--- implementation is wrong ---->\n",
        "\n",
        "      HC_loss = loss_fn(HC_prob, labels)\n",
        "      mPFC_loss = loss_fn(mPFC_prob, labels)\n",
        "\n",
        "      # Determine if HC or mPFC is more accurate\n",
        "      target_is_HC = HC_loss < mPFC_loss\n",
        "      target_is_HC = target_is_HC.float().view(-1, 1)  # Convert to float and reshape for broadcasting\n",
        "\n",
        "      # Calculate the BLA loss based on whether the prediction aligns with the more accurate model\n",
        "      BLA_target = target_is_HC\n",
        "      BLA_loss = nn.BCELoss()(class_prob, BLA_target)\n",
        "\n",
        "      # <--- implementation is wrong ---->\n",
        "\n",
        "      BLA_loss.backwards()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += BLA_loss\n",
        "\n",
        "    return (total_loss / (len(dataloader) * dataloader.batch_size))\n",
        "\n",
        "Bla = BLA()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_M6temMLhK_"
      },
      "source": [
        "# Dual Memory System Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_dX6yZDtixQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abccddf4-9659-43b4-802a-54f93e84d903"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DMSM()"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# dmsm\n",
        "\n",
        "class DMSM(nn.Module):\n",
        "  def __init__(self, input_dim=Input_Dim, classifier_dims = Classifier_Dims, learning_rate=Learning_Rate, p_dropout=Dropout, lambda_values = [1e4, 1.0, 0.1]):\n",
        "    super(DMSM, self).__init__()\n",
        "    self.mPFC = mPFC(input_dim, np.array([1024, 512]), classifier_dims, lambda_values, learning_rate)\n",
        "    self.HC = HC(input_dim, classifier_dims, learning_rate, p_dropout)\n",
        "    self.BLA()\n",
        "\n",
        "  def training(self, dataloader, HCepochs = HC_Epochs, BLAepochs = BLA_Epochs):\n",
        "    mPFC = self.mPFC\n",
        "    HC = self.HC\n",
        "    BLA = self.BLA\n",
        "\n",
        "    HC_loss_history = []\n",
        "    BLA_loss_history = []\n",
        "\n",
        "    # Train HC\n",
        "\n",
        "    for epoch in range(HC_Epochs):\n",
        "      loss = self.hc.back_prop(features, targets)\n",
        "      HC_loss_history.append(loss)\n",
        "\n",
        "\n",
        "    # Train BLA\n",
        "    for epoch in range(BLA_Epochs):\n",
        "      BLA.training_step\n",
        "\n",
        "  def sleep(self, dataloader, mPFC_Epochs = mPFC_Epochs):\n",
        "    mPFC = self.mPFC\n",
        "    HC = self.HC\n",
        "    BLA = self.BLA\n",
        "\n",
        "    mPFC_loss = []\n",
        "\n",
        "    # Integrate hallucinated images into the dataloader\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "\n",
        "\n",
        "    for epoch in range(mPFC_Epochs):\n",
        "      mPFC_Epochs.training_step(dataloader)\n",
        "\n",
        "  def eval(self, dataloader):\n",
        "    mPFC = self.mPFC\n",
        "    HC = self.HC\n",
        "    BLA = self.BLA\n",
        "\n",
        "    BLA.eval()\n",
        "    HC\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKJvA-5lt9N1"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "\n",
        "a = torch.tensor([1, 2, 3, 4])\n",
        "print(a[[0, 2]])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}